{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Journey From rank 30000 to 3852 in Housing Prices Competetion\n\nThis is my first public notebook where i want to share my journey from the first submission and scoring a rank of 30000 (might be off few hundreds) till the tenth submission where i managed to secure a rank of 3852.\n\nFirst things first:\n1. I was in search of free courses which could give me handson experience with Machine learning and thats when a friend of mine recommended the micro courses offered by Kaggle.com\n2. These micro courses set me up in the right direction. \nI started with **[Python](https://www.kaggle.com/learn/python)**. This is the prerequisite for most of the micro-courses.\nThis gave me the right foundation and helped me with the required coding skills. \n3. Then i took the course on **[Pandas](https://www.kaggle.com/learn/pandas)**. This course helped me to gain the required skills for data manipulation and data analysis.\n4. With the above two skills, i then took the **[Intro to Machine Learning](https://www.kaggle.com/learn/intro-to-machine-learning)**. By the end of this course, i was very excited as it guided me to make my first submission on Kaggle learn.\n5. The **[Intermediate Machine Learning](https://www.kaggle.com/learn/intermediate-machine-learning)** course helped me with key concepts (Handling missing values, Handling categorical values, Pipelines) for improving my score and securing a higher rank.\n\nThis notebook contains the practice code which i used in order to make my updated submission. I am continuing with the other micro courses and will continue to strive in order to improve my rank and ML skills.\n\nHope this helps the new ML enthusiasts to continue their AI/ML journey \n---\n"},{"metadata":{},"cell_type":"markdown","source":"As notified at the beginning, we will work with data from the [Housing Prices Competition for Kaggle Learn Users](https://www.kaggle.com/c/home-data-for-ml-course). \n\n![Ames Housing dataset image](https://i.imgur.com/lTJVG4e.png)\n\n"},{"metadata":{},"cell_type":"markdown","source":"# **Step 1: Loading the data** \n## Preparing the training, validation & test datasets"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n# Read the data\nX_full = pd.read_csv('../input/train.csv', index_col='Id')\nX_test_full = pd.read_csv('../input/test.csv', index_col='Id')\n\n# Remove rows with missing target, separate target from predictors\nX_full.dropna(axis=0, subset=['SalePrice'], inplace=True)\ny = X_full.SalePrice\nX_full.drop(['SalePrice'], axis=1, inplace=True)\n\n# Break off validation set from training data\nX_train_full, X_valid_full, y_train, y_valid = train_test_split(X_full, y, \n                                                                train_size=0.8, test_size=0.2,\n                                                                random_state=0)\n\n","execution_count":3,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Step 2: Data Cleansing\n**Identifying the Categorical Columns and Numerical Columns**\n1. Categorical Columns were selected based on the following conditions:\n    * (column - dtype = Object) and (cardinality(number of unique values) < 10) and (80% of the rows had data present)\n* Numerical Columns were selected based on the following conditions:\n    * (column - dtype is either 'int64' or 'float64') and (80 % of the rows had data present)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# threshold value was set. Columns with data less than the threshold value were dropped.\n\nthres = int(0.8*len(X_train_full)) # 934\n\n# \"Cardinality\" means the number of unique values in a column\n# Select categorical columns with relatively low cardinality and less missing values\ncategorical_cols = [cname for cname in X_train_full.columns if\n                    X_train_full[cname].nunique() < 10 and \n                    X_train_full[cname].dtype == \"object\" and\n                    X_train_full[cname].notna().sum() > thres]\n\n# Selecting numerical columns\nnumerical_cols = [cname for cname in X_train_full.columns if \n                X_train_full[cname].dtype in ['int64', 'float64'] and\n                X_train_full[cname].notna().sum() > thres]\n\n# Keep selected columns only\nmy_cols = categorical_cols + numerical_cols\nX_train = X_train_full[my_cols].copy()\nX_valid = X_valid_full[my_cols].copy()\nX_test = X_test_full[my_cols].copy()\n","execution_count":22,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Number of selected numerical columns: ',len(numerical_cols))\nprint('Number of selected categorical columns: ',len(categorical_cols))","execution_count":24,"outputs":[{"output_type":"stream","text":"Number of selected numerical columns:  36\nNumber of selected categorical columns:  35\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# identifying the columns with missing values. These missing data in the below columns will be imputed with values\n\nemp_cols_dict = {col:X_train[col].isnull().sum() for col in X_train.columns if X_train[col].isnull().any()}\nemp_cols_dict","execution_count":23,"outputs":[{"output_type":"execute_result","execution_count":23,"data":{"text/plain":"{'MasVnrType': 6,\n 'BsmtQual': 28,\n 'BsmtCond': 28,\n 'BsmtExposure': 28,\n 'BsmtFinType1': 28,\n 'BsmtFinType2': 29,\n 'Electrical': 1,\n 'GarageType': 58,\n 'GarageFinish': 58,\n 'GarageQual': 58,\n 'GarageCond': 58,\n 'LotFrontage': 212,\n 'MasVnrArea': 6,\n 'GarageYrBlt': 58}"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"# Step 3 - Data Preprocessing\n\n1. For preprocessing of numerical data - we will impute the data using the SimpleImputer()\n2. For preprocessing of categorical data - we will impute the data using SimpleImputer() and then encode the data using OneHotEncoder()\n3. Pipelines will be used to organize the sequential processing of the columnar data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# importing the necessary libraries for preprocessing, building a pipeline.\n\n# Hyperparameter Tuning for the SimpleImputer strategy with Median yielded better results\n\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\nfrom xgboost import XGBRegressor\n\n# Preprocessing for numerical data using SimpleImputer()\nnumerical_transformer = SimpleImputer(strategy='median') \n\n# Preprocessing for categorical data using SimpleImputer() and OneHotEncoder()\ncategorical_transformer = Pipeline(steps = [\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\n# Bundle preprocessing for numerical and categorical data using a ColumnTransformer()\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numerical_transformer, numerical_cols),\n        ('cat', categorical_transformer, categorical_cols)\n    ])\n\n","execution_count":33,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Step 4: Model definition and generating predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# Define model\n\n# Using the XG boost regressor model to estimate the model parameter and make predictions \n# I experimented with the values of n_estimators ranging from 100-1000 and \n# learning_rate values from 0.1-0.09 in order to identify the best accuracy\n\nmodel = XGBRegressor(n_estimators=750, learning_rate = 0.06,random_state=0)\n\n\n# Bundle the preprocessing and modeling code in a pipeline\nmy_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n                              ('model', model)\n                             ])\n\n# Preprocessing of training data, fit model \nmy_pipeline.fit(X_train, y_train)\n\n# Preprocessing of validation data, get predictions\npreds = my_pipeline.predict(X_valid)\n\n# Evaluate the model\nscore = mean_absolute_error(y_valid, preds)\nprint('MAE:', score)\n\n","execution_count":34,"outputs":[{"output_type":"stream","text":"MAE: 16569.299737799658\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# Step 5: Generate test predictions and preparation of the submission data file\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Preprocessing of test data, fit model\npreds_test = my_pipeline.predict(X_test) # Your code here\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Save test predictions to file\noutput = pd.DataFrame({'Id': X_test.index,\n                       'SalePrice': preds_test})\noutput.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Key Notes\n\nThis notebook is meant for encouraging the new ML enthusiasts by showing a way to improve ranking in the Housing prices competetion.\nThe notebook has room for improvement. There are several other techniques which have to be mastered by me in order to move up in the rankings. I will continue with my journey and wish all the new ML enthusiasts \"happy learning\".\n"}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}