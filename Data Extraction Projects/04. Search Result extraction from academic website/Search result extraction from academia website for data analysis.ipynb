{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script to extract data from the url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 1 - Data Extraction\n",
    "\n",
    "### Modules to be used for data extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib import request as req\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import lxml\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing the website address for data extraction\n",
    "\n",
    "url = 'https://arxiv.org/search/?query=Vq+VAE&source=header&searchtype=all'\n",
    "url2 = 'https://openreview.net/search?term=ICML++vae&content=all&group=all&source=all'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 - Connection Establishment\n",
    "   >This step comprises of three sub steps:\n",
    " - Opening of client and establishing connection\n",
    " - Reading the page content\n",
    " - Closing the client and connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# opening the client connection\n",
    "client = urlopen(url) \n",
    "\n",
    "# reading the data from the html page and storing it\n",
    "page_html = client.read()\n",
    "\n",
    "# closing the client connection\n",
    "client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 - Parsing of the web information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the extracted page is parsed using BeautifulSoup\n",
    "page_soup = bs(page_html, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<!DOCTYPE html>\n",
       "<html lang=\"en\">\n",
       "<head>\n",
       "<meta charset=\"utf-8\"/>\n",
       "<meta content=\"width=device-width, initial-scale=1\" name=\"viewport\"/>\n",
       "<!-- new favicon config and versions by realfavicongenerator.net -->\n",
       "<link href=\"https://static.arxiv.org/static/base/0.16.8/images/icons/apple-touch-icon.png\" rel=\"apple-touch-icon\" sizes=\"180x180\"/>\n",
       "<link href=\"https://static.arxiv.org/static/base/0.16.8/images/icons/favicon-32x32.png\" rel=\"icon\" sizes=\"32x32\" type=\"image/png\"/>\n",
       "<link href=\"https://static.arxiv.org/static/base/0.16.8/images/icons/favicon-16x16.png\" rel=\"icon\" sizes=\"16x16\" type=\"image/png\"/>\n",
       "<link href=\"https://static.arxiv.org/static/base/0.16.8/images/icons/site.webmanifest\" rel=\"manifest\"/>\n",
       "<link color=\"#b31b1b\" href=\"https://static.arxiv.org/static/base/0.16.8/images/icons/safari-pinned-tab.svg\" rel=\"mask-icon\"/>\n",
       "<link href=\"https://static.arxiv.org/static/base/0.16.8/images/icons/favicon.ico\" rel=\"shortcut icon\"/>\n",
       "<meta content=\"#b31b1b\" name=\"msapplication-TileColor\"/>\n",
       "<meta content=\"images/icons/browserconfig.xml\" name=\"msapplication-config\"/>\n",
       "<meta content=\"#b31b1b\" name=\"theme-color\"/>\n",
       "<!-- end favicon config -->\n",
       "<title>Search | arXiv e-print repository</title>\n",
       "<script defer=\"\" src=\"https://static.arxiv.org/static/base/0.16.8/fontawesome-free-5.11.2-web/js/all.js\"></script>\n",
       "<link href=\"https://static.arxiv.org/static/base/0.16.8/css/arxivstyle.css\" rel=\"stylesheet\"/>\n",
       "<script type=\"text/x-mathjax-config\">\n",
       "  MathJax.Hub.Config({\n",
       "    messageStyle: \"none\",\n",
       "    extensions: [\"tex2jax.js\"],\n",
       "    jax: [\"input/TeX\", \"output/HTML-CSS\"],\n",
       "    tex2jax: {\n",
       "      inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ],\n",
       "      displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ],\n",
       "      processEscapes: true,\n",
       "      ignoreClass: '.*',\n",
       "      processClass: 'mathjax.*'\n",
       "    },\n",
       "    TeX: {\n",
       "        extensions: [\"AMSmath.js\", \"AMSsymbols.js\", \"noErrors.js\"],\n",
       "        noErrors: {\n",
       "          inlineDelimiters: [\"$\",\"$\"],\n",
       "          multiLine: false,\n",
       "          style: {\n",
       "            \"font-size\": \"normal\",\n",
       "            \"border\": \"\"\n",
       "          }\n",
       "        }\n",
       "    },\n",
       "    \"HTML-CSS\": { availableFonts: [\"TeX\"] }\n",
       "  });\n",
       "</script>\n",
       "<script src=\"//static.arxiv.org/MathJax-2.7.3/MathJax.js\"></script>\n",
       "<script src=\"https://static.arxiv.org/static/base/0.16.8/js/notification.js\"></script>\n",
       "<link href=\"https://static.arxiv.org/static/search/0.5.6/css/bulma-tooltip.min.css\" rel=\"stylesheet\"/>\n",
       "<link href=\"https://static.arxiv.org/static/search/0.5.6/css/search.css\" rel=\"stylesheet\"/>\n",
       "<script crossorigin=\"anonymous\" integrity=\"sha256-k2WSCIexGzOj3Euiig+TlR8gA0EmPjuc79OEeY5L45g=\" src=\"https://code.jquery.com/jquery-3.2.1.slim.min.js\"></script>\n",
       "<script src=\"https://static.arxiv.org/static/search/0.5.6/js/fieldset.js\"></script>\n",
       "<style>\n",
       "  radio#cf-customfield_11400 {\n",
       "    display: none;\n",
       "  }\n",
       "  </style>\n",
       "<script src=\"https://arxiv-org.atlassian.net/s/d41d8cd98f00b204e9800998ecf8427e-T/-tqqyqk/b/20/a44af77267a987a660377e5c46e0fb64/_/download/batch/com.atlassian.jira.collector.plugin.jira-issue-collector-plugin:issuecollector/com.atlassian.jira.collector.plugin.jira-issue-collector-plugin:issuecollector.js?locale=en-US&amp;collectorId=3b3dcb4c\" type=\"text/javascript\"></script>\n",
       "<script type=\"text/javascript\">\n",
       "    window.ATL_JQ_PAGE_PROPS =  {\n",
       "    \t\"triggerFunction\": function(showCollectorDialog) {\n",
       "    \t\t//Requires that jQuery is available!\n",
       "    \t\t$(\"#feedback-button\").click(function(e) {\n",
       "    \t\t\te.preventDefault();\n",
       "    \t\t\tshowCollectorDialog();\n",
       "    \t\t});\n",
       "    \t},\n",
       "      fieldValues: {\n",
       "        \"components\": [\"16000\"],  // Search component.\n",
       "        \"versions\": [\"14260\"],  // Release search-0.5.6\n",
       "        \"customfield_11401\": window.location.href\n",
       "      }\n",
       "    };\n",
       "    </script>\n",
       "<!-- Pendo -->\n",
       "<script>\n",
       "     (function(apiKey){\n",
       "         (function(p,e,n,d,o){var v,w,x,y,z;o=p[d]=p[d]||{};o._q=[];\n",
       "             v=['initialize','identify','updateOptions','pageLoad'];for(w=0,x=v.length;w<x;++w)(function(m){\n",
       "                 o[m]=o[m]||function(){o._q[m===v[0]?'unshift':'push']([m].concat([].slice.call(arguments,0)));};})(v[w]);\n",
       "             y=e.createElement(n);y.async=!0;y.src='https://content.analytics.arxiv.org/agent/static/'+apiKey+'/pendo.js';\n",
       "             z=e.getElementsByTagName(n)[0];z.parentNode.insertBefore(y,z);})(window,document,'script','pendo');\n",
       "\n",
       "         // Call this whenever information about your visitors becomes available\n",
       "         // Please use Strings, Numbers, or Bools for value types.\n",
       "         pendo.initialize({\n",
       "             visitor: {\n",
       "                 id:              'VISITOR-UNIQUE-ID'   // Required if user is logged in\n",
       "                 // email:        // Recommended if using Pendo Feedback, or NPS Email\n",
       "                 // full_name:    // Recommended if using Pendo Feedback\n",
       "                 // role:         // Optional\n",
       "\n",
       "                 // You can add any additional visitor level key-values here,\n",
       "                 // as long as it's not one of the above reserved names.\n",
       "             },\n",
       "\n",
       "             account: {\n",
       "                 id:           'ACCOUNT-UNIQUE-ID' // Highly recommended\n",
       "                 // name:         // Optional\n",
       "                 // is_paying:    // Recommended if using Pendo Feedback\n",
       "                 // monthly_value:// Recommended if using Pendo Feedback\n",
       "                 // planLevel:    // Optional\n",
       "                 // planPrice:    // Optional\n",
       "                 // creationDate: // Optional\n",
       "\n",
       "                 // You can add any additional account level key-values here,\n",
       "                 // as long as it's not one of the above reserved names.\n",
       "             }\n",
       "         });\n",
       "     })('d6494389-b427-4103-7c76-03182ecc8e60');\n",
       "    </script>\n",
       "<!-- End Pendo -->\n",
       "</head>\n",
       "<body>\n",
       "<header><a class=\"is-sr-only\" href=\"#main-container\">Skip to main content</a>\n",
       "<!-- contains Cornell logo and sponsor statement -->\n",
       "<div class=\"attribution level is-marginless\" role=\"banner\">\n",
       "<div class=\"level-left\">\n",
       "<a class=\"level-item\" href=\"https://cornell.edu/\"><img alt=\"Cornell University\" aria-label=\"logo\" src=\"https://static.arxiv.org/static/base/0.16.8/images/cornell-reduced-white-SMALL.svg\" width=\"200\"/></a>\n",
       "</div>\n",
       "<div class=\"level-right is-marginless\"><p class=\"sponsors level-item is-marginless\"><a href=\"https://confluence.cornell.edu/x/ALlRF\">We gratefully acknowledge support from<br/> the Simons Foundation and member institutions.</a></p></div>\n",
       "</div>\n",
       "<!-- contains arXiv identity and search bar -->\n",
       "<div class=\"identity level is-marginless\">\n",
       "<div class=\"level-left\">\n",
       "<div class=\"level-item\">\n",
       "<a aria-label=\"arxiv-logo\" class=\"arxiv\" href=\"https://arxiv.org/\"><img alt=\"arXiv\" aria-label=\"logo\" src=\"https://static.arxiv.org/static/base/0.16.8/images/arxiv-logo-web.svg\" width=\"85\"/></a>\n",
       "</div>\n",
       "</div>\n",
       "<div class=\"search-block level-right\">\n",
       "<form action=\"https://arxiv.org/search\" class=\"level-item mini-search\" method=\"GET\">\n",
       "<div class=\"field has-addons\">\n",
       "<div class=\"control\">\n",
       "<input aria-label=\"Search term or terms\" class=\"input is-small\" name=\"query\" placeholder=\"Search...\" type=\"text\"/>\n",
       "<p class=\"help\"><a href=\"https://arxiv.org/help\">Help</a> | <a href=\"https://arxiv.org/search/advanced\">Advanced Search</a></p>\n",
       "</div>\n",
       "<div class=\"control\">\n",
       "<div class=\"select is-small\">\n",
       "<select aria-label=\"Field to search\" name=\"searchtype\">\n",
       "<option selected=\"selected\" value=\"all\">All fields</option>\n",
       "<option value=\"title\">Title</option>\n",
       "<option value=\"author\">Author</option>\n",
       "<option value=\"abstract\">Abstract</option>\n",
       "<option value=\"comments\">Comments</option>\n",
       "<option value=\"journal_ref\">Journal reference</option>\n",
       "<option value=\"acm_class\">ACM classification</option>\n",
       "<option value=\"msc_class\">MSC classification</option>\n",
       "<option value=\"report_num\">Report number</option>\n",
       "<option value=\"paper_id\">arXiv identifier</option>\n",
       "<option value=\"doi\">DOI</option>\n",
       "<option value=\"orcid\">ORCID</option>\n",
       "<option value=\"author_id\">arXiv author ID</option>\n",
       "<option value=\"help\">Help pages</option>\n",
       "<option value=\"full_text\">Full text</option>\n",
       "</select>\n",
       "</div>\n",
       "</div>\n",
       "<input name=\"source\" type=\"hidden\" value=\"header\"/>\n",
       "<button class=\"button is-small is-cul-darker\">Search</button>\n",
       "</div>\n",
       "</form>\n",
       "</div>\n",
       "</div> <!-- closes identity -->\n",
       "<div class=\"container\">\n",
       "<div aria-label=\"User menu\" class=\"user-tools is-size-7 has-text-right has-text-weight-bold\" role=\"navigation\">\n",
       "<a href=\"https://arxiv.org/login\">Login</a>\n",
       "</div>\n",
       "</div>\n",
       "</header>\n",
       "<main class=\"container\" id=\"main-container\">\n",
       "<div class=\"level is-marginless\">\n",
       "<div class=\"level-left\">\n",
       "<h1 class=\"title is-clearfix\">\n",
       "    \n",
       "        Showing 1–31 of 31 results for all: <span class=\"mathjax\">Vq VAE</span>\n",
       "</h1>\n",
       "</div>\n",
       "<div class=\"level-right is-hidden-mobile\">\n",
       "<!-- feedback for mobile is moved to footer -->\n",
       "<span class=\"help\" style=\"display: inline-block;\"><a href=\"https://github.com/arXiv/arxiv-search/releases\">Search v0.5.6 released 2020-02-24</a>  </span>\n",
       "<button class=\"button is-small\" id=\"feedback-button\">Feedback?</button>\n",
       "</div>\n",
       "</div>\n",
       "<div class=\"content\">\n",
       "<form action=\"/search/\" aria-role=\"search\" method=\"GET\">\n",
       "<div class=\"field has-addons-tablet\">\n",
       "<div class=\"control is-expanded\">\n",
       "<label class=\"hidden-label\" for=\"query\">Search term or terms</label>\n",
       "<input class=\"input is-medium\" id=\"query\" name=\"query\" placeholder=\"Search term...\" type=\"text\" value=\"Vq VAE\"/>\n",
       "</div>\n",
       "<div class=\"select control is-medium\">\n",
       "<label class=\"is-hidden\" for=\"searchtype\">Field</label>\n",
       "<select class=\"is-medium\" id=\"searchtype\" name=\"searchtype\"><option selected=\"\" value=\"all\">All fields</option><option value=\"title\">Title</option><option value=\"author\">Author(s)</option><option value=\"abstract\">Abstract</option><option value=\"comments\">Comments</option><option value=\"journal_ref\">Journal reference</option><option value=\"acm_class\">ACM classification</option><option value=\"msc_class\">MSC classification</option><option value=\"report_num\">Report number</option><option value=\"paper_id\">arXiv identifier</option><option value=\"doi\">DOI</option><option value=\"orcid\">ORCID</option><option value=\"license\">License (URI)</option><option value=\"author_id\">arXiv author ID</option><option value=\"help\">Help pages</option><option value=\"full_text\">Full text</option></select>\n",
       "</div>\n",
       "<div class=\"control\">\n",
       "<button class=\"button is-link is-medium\">Search</button>\n",
       "</div>\n",
       "</div>\n",
       "<div class=\"field\">\n",
       "<div class=\"control is-size-7\">\n",
       "<label class=\"radio\">\n",
       "<input checked=\"\" id=\"abstracts-0\" name=\"abstracts\" type=\"radio\" value=\"show\"/> Show abstracts\n",
       "        </label>\n",
       "<label class=\"radio\">\n",
       "<input id=\"abstracts-1\" name=\"abstracts\" type=\"radio\" value=\"hide\"/> Hide abstracts\n",
       "        </label>\n",
       "</div>\n",
       "</div>\n",
       "<div class=\"is-clearfix\" style=\"height: 2.5em\">\n",
       "<div class=\"is-pulled-right\">\n",
       "<a href=\"/search/advanced?terms-0-term=Vq+VAE&amp;terms-0-field=all&amp;size=50&amp;order=-announced_date_first\">Advanced Search</a>\n",
       "</div>\n",
       "</div>\n",
       "<input name=\"order\" type=\"hidden\" value=\"-announced_date_first\"/>\n",
       "<input name=\"size\" type=\"hidden\" value=\"50\"/>\n",
       "</form>\n",
       "<div class=\"level breathe-horizontal\">\n",
       "<div class=\"level-left\">\n",
       "<form action=\"/search/\" method=\"GET\">\n",
       "<div style=\"display: none;\">\n",
       "<select id=\"searchtype\" name=\"searchtype\"><option selected=\"\" value=\"all\">All fields</option><option value=\"title\">Title</option><option value=\"author\">Author(s)</option><option value=\"abstract\">Abstract</option><option value=\"comments\">Comments</option><option value=\"journal_ref\">Journal reference</option><option value=\"acm_class\">ACM classification</option><option value=\"msc_class\">MSC classification</option><option value=\"report_num\">Report number</option><option value=\"paper_id\">arXiv identifier</option><option value=\"doi\">DOI</option><option value=\"orcid\">ORCID</option><option value=\"license\">License (URI)</option><option value=\"author_id\">arXiv author ID</option><option value=\"help\">Help pages</option><option value=\"full_text\">Full text</option></select>\n",
       "<input id=\"query\" name=\"query\" type=\"text\" value=\"Vq VAE\"/>\n",
       "<ul id=\"abstracts\"><li><input checked=\"\" id=\"abstracts-0\" name=\"abstracts\" type=\"radio\" value=\"show\"/> <label for=\"abstracts-0\">Show abstracts</label></li><li><input id=\"abstracts-1\" name=\"abstracts\" type=\"radio\" value=\"hide\"/> <label for=\"abstracts-1\">Hide abstracts</label></li></ul>\n",
       "</div>\n",
       "<div class=\"box field is-grouped is-grouped-multiline level-item\">\n",
       "<div class=\"control\">\n",
       "<span class=\"select is-small\">\n",
       "<select id=\"size\" name=\"size\"><option value=\"25\">25</option><option selected=\"\" value=\"50\">50</option><option value=\"100\">100</option><option value=\"200\">200</option></select>\n",
       "</span>\n",
       "<label for=\"size\">results per page</label>.\n",
       "        </div>\n",
       "<div class=\"control\">\n",
       "<label for=\"order\">Sort results by</label>\n",
       "<span class=\"select is-small\">\n",
       "<select id=\"order\" name=\"order\"><option selected=\"\" value=\"-announced_date_first\">Announcement date (newest first)</option><option value=\"announced_date_first\">Announcement date (oldest first)</option><option value=\"-submitted_date\">Submission date (newest first)</option><option value=\"submitted_date\">Submission date (oldest first)</option><option value=\"\">Relevance</option></select>\n",
       "</span>\n",
       "</div>\n",
       "<div class=\"control\">\n",
       "<button class=\"button is-small is-link\">Go</button>\n",
       "</div>\n",
       "</div>\n",
       "</form>\n",
       "</div>\n",
       "</div>\n",
       "<ol class=\"breathe-horizontal\" start=\"1\">\n",
       "<li class=\"arxiv-result\">\n",
       "<div class=\"is-marginless\">\n",
       "<p class=\"list-title is-inline-block\"><a href=\"https://arxiv.org/abs/2008.04549\">arXiv:2008.04549</a>\n",
       "<span> [<a href=\"https://arxiv.org/pdf/2008.04549\">pdf</a>, <a href=\"https://arxiv.org/format/2008.04549\">other</a>] </span>\n",
       "</p>\n",
       "<div class=\"tags is-inline-block\">\n",
       "<span class=\"tag is-small is-link tooltip is-tooltip-top\" data-tooltip=\"Audio and Speech Processing\">eess.AS</span>\n",
       "<span class=\"tag is-small is-grey tooltip is-tooltip-top\" data-tooltip=\"Sound\">cs.SD</span>\n",
       "</div>\n",
       "</div>\n",
       "<p class=\"title is-5 mathjax\">\n",
       "      \n",
       "        Unsupervised Learning For Sequence-to-sequence Text-to-speech For Low-resource Languages\n",
       "      \n",
       "    </p>\n",
       "<p class=\"authors\">\n",
       "<span class=\"has-text-black-bis has-text-weight-semibold\">Authors:</span>\n",
       "<a href=\"/search/?searchtype=author&amp;query=Zhang%2C+H\">Haitong Zhang</a>, \n",
       "      \n",
       "      <a href=\"/search/?searchtype=author&amp;query=Lin%2C+Y\">Yue Lin</a>\n",
       "</p>\n",
       "<p class=\"abstract mathjax\">\n",
       "<span class=\"search-hit\">Abstract</span>:\n",
       "      <span class=\"abstract-short has-text-grey-dark mathjax\" id=\"2008.04549v1-abstract-short\" style=\"display: inline;\">\n",
       "        …the problem of heavy data demand, we propose a novel unsupervised pre-training mechanism in this paper. Specifically, we first use Vector-quantization Variational-Autoencoder (<span class=\"search-hit mathjax\">VQ</span>-<span class=\"search-hit mathjax\">VAE</span>) to ex-tract the unsupervised linguistic units from large-scale, publicly found, and untranscribed speech. We then pre-train the sequence…\n",
       "        <a class=\"is-size-7\" onclick=\"document.getElementById('2008.04549v1-abstract-full').style.display = 'inline'; document.getElementById('2008.04549v1-abstract-short').style.display = 'none';\" style=\"white-space: nowrap;\">▽ More</a>\n",
       "</span>\n",
       "<span class=\"abstract-full has-text-grey-dark mathjax\" id=\"2008.04549v1-abstract-full\" style=\"display: none;\">\n",
       "        Recently, sequence-to-sequence models with attention have been successfully applied in Text-to-speech (TTS). These models can generate near-human speech with a large accurately-transcribed speech corpus. However, preparing such a large data-set is both expensive and laborious. To alleviate the problem of heavy data demand, we propose a novel unsupervised pre-training mechanism in this paper. Specifically, we first use Vector-quantization Variational-Autoencoder (<span class=\"search-hit mathjax\">VQ</span>-<span class=\"search-hit mathjax\">VAE</span>) to ex-tract the unsupervised linguistic units from large-scale, publicly found, and untranscribed speech. We then pre-train the sequence-to-sequence TTS model by using the&lt;unsupervised linguistic units, audio&gt;pairs. Finally, we fine-tune the model with a small amount of&lt;text, audio&gt;paired data from the target speaker. As a result, both objective and subjective evaluations show that our proposed method can synthesize more intelligible and natural speech with the same amount of paired training data. Besides, we extend our proposed method to the hypothesized low-resource languages and verify the effectiveness of the method using objective evaluation.\n",
       "        <a class=\"is-size-7\" onclick=\"document.getElementById('2008.04549v1-abstract-full').style.display = 'none'; document.getElementById('2008.04549v1-abstract-short').style.display = 'inline';\" style=\"white-space: nowrap;\">△ Less</a>\n",
       "</span>\n",
       "</p>\n",
       "<p class=\"is-size-7\"><span class=\"has-text-black-bis has-text-weight-semibold\">Submitted</span> 11 August, 2020; \n",
       "      <span class=\"has-text-black-bis has-text-weight-semibold\">originally announced</span> August 2020.\n",
       "      \n",
       "    </p>\n",
       "<p class=\"comments is-size-7\">\n",
       "<span class=\"has-text-black-bis has-text-weight-semibold\">Comments:</span>\n",
       "<span class=\"has-text-grey-dark mathjax\">Accepted to the conference of INTERSPEECH 2020</span>\n",
       "</p>\n",
       "</li>\n",
       "<li class=\"arxiv-result\">\n",
       "<div class=\"is-marginless\">\n",
       "<p class=\"list-title is-inline-block\"><a href=\"https://arxiv.org/abs/2008.02528\">arXiv:2008.02528</a>\n",
       "<span> [<a href=\"https://arxiv.org/pdf/2008.02528\">pdf</a>, <a href=\"https://arxiv.org/format/2008.02528\">other</a>] </span>\n",
       "</p>\n",
       "<div class=\"tags is-inline-block\">\n",
       "<span class=\"tag is-small is-link tooltip is-tooltip-top\" data-tooltip=\"Machine Learning\">cs.LG</span>\n",
       "<span class=\"tag is-small is-grey tooltip is-tooltip-top\" data-tooltip=\"Machine Learning\">stat.ML</span>\n",
       "</div>\n",
       "</div>\n",
       "<p class=\"title is-5 mathjax\">\n",
       "      \n",
       "        Learning Sampling in Financial Statement Audits using Vector Quantised Autoencoder Neural Networks\n",
       "      \n",
       "    </p>\n",
       "<p class=\"authors\">\n",
       "<span class=\"has-text-black-bis has-text-weight-semibold\">Authors:</span>\n",
       "<a href=\"/search/?searchtype=author&amp;query=Schreyer%2C+M\">Marco Schreyer</a>, \n",
       "      \n",
       "      <a href=\"/search/?searchtype=author&amp;query=Sattarov%2C+T\">Timur Sattarov</a>, \n",
       "      \n",
       "      <a href=\"/search/?searchtype=author&amp;query=Gierbl%2C+A\">Anita Gierbl</a>, \n",
       "      \n",
       "      <a href=\"/search/?searchtype=author&amp;query=Reimer%2C+B\">Bernd Reimer</a>, \n",
       "      \n",
       "      <a href=\"/search/?searchtype=author&amp;query=Borth%2C+D\">Damian Borth</a>\n",
       "</p>\n",
       "<p class=\"abstract mathjax\">\n",
       "<span class=\"search-hit\">Abstract</span>:\n",
       "      <span class=\"abstract-short has-text-grey-dark mathjax\" id=\"2008.02528v1-abstract-short\" style=\"display: inline;\">\n",
       "        …and their dynamics that resulted in the journal entries in-scope of the audit. To overcome this challenge, we propose the application of Vector Quantised-Variational Autoencoder (<span class=\"search-hit mathjax\">VQ</span>-<span class=\"search-hit mathjax\">VAE</span>) neural networks. We demonstrate, based on two real-world city payment datasets, that such artificial neural networks are capable of l…\n",
       "        <a class=\"is-size-7\" onclick=\"document.getElementById('2008.02528v1-abstract-full').style.display = 'inline'; document.getElementById('2008.02528v1-abstract-short').style.display = 'none';\" style=\"white-space: nowrap;\">▽ More</a>\n",
       "</span>\n",
       "<span class=\"abstract-full has-text-grey-dark mathjax\" id=\"2008.02528v1-abstract-full\" style=\"display: none;\">\n",
       "        The audit of financial statements is designed to collect reasonable assurance that an issued statement is free from material misstatement 'true and fair presentation'. International audit standards require the assessment of a statements' underlying accounting relevant transactions referred to as 'journal entries' to detect potential misstatements. To efficiently audit the increasing quantities of such entries, auditors regularly conduct a sample-based assessment referred to as 'audit sampling'. However, the task of audit sampling is often conducted early in the overall audit process. Often at a stage, in which an auditor might be unaware of all generative factors and their dynamics that resulted in the journal entries in-scope of the audit. To overcome this challenge, we propose the application of Vector Quantised-Variational Autoencoder (<span class=\"search-hit mathjax\">VQ</span>-<span class=\"search-hit mathjax\">VAE</span>) neural networks. We demonstrate, based on two real-world city payment datasets, that such artificial neural networks are capable of learning a quantised representation of accounting data. We show that the learned quantisation uncovers (i) the latent factors of variation and (ii) can be utilised as a highly representative audit sample in financial statement audits.\n",
       "        <a class=\"is-size-7\" onclick=\"document.getElementById('2008.02528v1-abstract-full').style.display = 'none'; document.getElementById('2008.02528v1-abstract-short').style.display = 'inline';\" style=\"white-space: nowrap;\">△ Less</a>\n",
       "</span>\n",
       "</p>\n",
       "<p class=\"is-size-7\"><span class=\"has-text-black-bis has-text-weight-semibold\">Submitted</span> 6 August, 2020; \n",
       "      <span class=\"has-text-black-bis has-text-weight-semibold\">originally announced</span> August 2020.\n",
       "      \n",
       "    </p>\n",
       "<p class=\"comments is-size-7\">\n",
       "<span class=\"has-text-black-bis has-text-weight-semibold\">Comments:</span>\n",
       "<span class=\"has-text-grey-dark mathjax\">8 pages, 5 figures, 3 tables, to appear in Proceedings of the ACM's International Conference on AI in Finance (ICAIF'20), this paper is the initial accepted version</span>\n",
       "</p>\n",
       "</li>\n",
       "<li class=\"arxiv-result\">\n",
       "<div class=\"is-marginless\">\n",
       "<p class=\"list-title is-inline-block\"><a href=\"https://arxiv.org/abs/2007.09923\">arXiv:2007.09923</a>\n",
       "<span> [<a href=\"https://arxiv.org/pdf/2007.09923\">pdf</a>, <a href=\"https://arxiv.org/format/2007.09923\">other</a>] </span>\n",
       "</p>\n",
       "<div class=\"tags is-inline-block\">\n",
       "<span class=\"tag is-small is-link tooltip is-tooltip-top\" data-tooltip=\"Computer Vision and Pattern Recognition\">cs.CV</span>\n",
       "<span class=\"tag is-small is-grey tooltip is-tooltip-top\" data-tooltip=\"Machine Learning\">cs.LG</span>\n",
       "<span class=\"tag is-small is-grey tooltip is-tooltip-top\" data-tooltip=\"Image and Video Processing\">eess.IV</span>\n",
       "</div>\n",
       "</div>\n",
       "<p class=\"title is-5 mathjax\">\n",
       "      \n",
       "        Incorporating Reinforced Adversarial Learning in Autoregressive Image Generation\n",
       "      \n",
       "    </p>\n",
       "<p class=\"authors\">\n",
       "<span class=\"has-text-black-bis has-text-weight-semibold\">Authors:</span>\n",
       "<a href=\"/search/?searchtype=author&amp;query=Ak%2C+K+E\">Kenan E. Ak</a>, \n",
       "      \n",
       "      <a href=\"/search/?searchtype=author&amp;query=Xu%2C+N\">Ning Xu</a>, \n",
       "      \n",
       "      <a href=\"/search/?searchtype=author&amp;query=Lin%2C+Z\">Zhe Lin</a>, \n",
       "      \n",
       "      <a href=\"/search/?searchtype=author&amp;query=Wang%2C+Y\">Yilin Wang</a>\n",
       "</p>\n",
       "<p class=\"abstract mathjax\">\n",
       "<span class=\"search-hit\">Abstract</span>:\n",
       "      <span class=\"abstract-short has-text-grey-dark mathjax\" id=\"2007.09923v1-abstract-short\" style=\"display: inline;\">\n",
       "        …models recently achieved comparable results versus state-of-the-art Generative Adversarial Networks (GANs) with the help of Vector Quantized Variational AutoEncoders (<span class=\"search-hit mathjax\">VQ</span>-…\n",
       "        <a class=\"is-size-7\" onclick=\"document.getElementById('2007.09923v1-abstract-full').style.display = 'inline'; document.getElementById('2007.09923v1-abstract-short').style.display = 'none';\" style=\"white-space: nowrap;\">▽ More</a>\n",
       "</span>\n",
       "<span class=\"abstract-full has-text-grey-dark mathjax\" id=\"2007.09923v1-abstract-full\" style=\"display: none;\">\n",
       "        Autoregressive models recently achieved comparable results versus state-of-the-art Generative Adversarial Networks (GANs) with the help of Vector Quantized Variational AutoEncoders (<span class=\"search-hit mathjax\">VQ</span>-<span class=\"search-hit mathjax\">VAE</span>). However, autoregressive models have several limitations such as exposure bias and their training objective does not guarantee visual fidelity. To address these limitations, we propose to use Reinforced Adversarial Learning (RAL) based on policy gradient optimization for autoregressive models. By applying RAL, we enable a similar process for training and testing to address the exposure bias issue. In addition, visual fidelity has been further optimized with adversarial loss inspired by their strong counterparts: GANs. Due to the slow sampling speed of autoregressive models, we propose to use partial generation for faster training. RAL also empowers the collaboration between different modules of the <span class=\"search-hit mathjax\">VQ</span>-<span class=\"search-hit mathjax\">VAE</span> framework. To our best knowledge, the proposed method is first to enable adversarial learning in autoregressive models for image generation. Experiments on synthetic and real-world datasets show improvements over the MLE trained models. The proposed method improves both negative log-likelihood (NLL) and Fréchet Inception Distance (FID), which indicates improvements in terms of visual quality and diversity. The proposed method achieves state-of-the-art results on Celeba for 64 $\\times$ 64 image resolution, showing promise for large scale image generation.\n",
       "        <a class=\"is-size-7\" onclick=\"document.getElementById('2007.09923v1-abstract-full').style.display = 'none'; document.getElementById('2007.09923v1-abstract-short').style.display = 'inline';\" style=\"white-space: nowrap;\">△ Less</a>\n",
       "</span>\n",
       "</p>\n",
       "<p class=\"is-size-7\"><span class=\"has-text-black-bis has-text-weight-semibold\">Submitted</span> 20 July, 2020; \n",
       "      <span class=\"has-text-black-bis has-text-weight-semibold\">originally announced</span> July 2020.\n",
       "      \n",
       "    </p>\n",
       "<p class=\"comments is-size-7\">\n",
       "<span class=\"has-text-black-bis has-text-weight-semibold\">Comments:</span>\n",
       "<span class=\"has-text-grey-dark mathjax\">Accepted to ECCV 2020</span>\n",
       "</p>\n",
       "</li>\n",
       "<li class=\"arxiv-result\">\n",
       "<div class=\"is-marginless\">\n",
       "<p class=\"list-title is-inline-block\"><a href=\"https://arxiv.org/abs/2006.12150\">arXiv:2006.12150</a>\n",
       "<span> [<a href=\"https://arxiv.org/pdf/2006.12150\">pdf</a>, <a href=\"https://arxiv.org/format/2006.12150\">other</a>] </span>\n",
       "</p>\n",
       "<div class=\"tags is-inline-block\">\n",
       "<span class=\"tag is-small is-link tooltip is-tooltip-top\" data-tooltip=\"Computer Vision and Pattern Recognition\">cs.CV</span>\n",
       "</div>\n",
       "</div>\n",
       "<p class=\"title is-5 mathjax\">\n",
       "      \n",
       "        Generating Annotated High-Fidelity Images Containing Multiple Coherent Objects\n",
       "      \n",
       "    </p>\n",
       "<p class=\"authors\">\n",
       "<span class=\"has-text-black-bis has-text-weight-semibold\">Authors:</span>\n",
       "<a href=\"/search/?searchtype=author&amp;query=Cardenas%2C+B+G\">Bryan G. Cardenas</a>, \n",
       "      \n",
       "      <a href=\"/search/?searchtype=author&amp;query=Arya%2C+D\">Devanshu Arya</a>, \n",
       "      \n",
       "      <a href=\"/search/?searchtype=author&amp;query=Gupta%2C+D+K\">Deepak K. Gupta</a>\n",
       "</p>\n",
       "<p class=\"abstract mathjax\">\n",
       "<span class=\"search-hit\">Abstract</span>:\n",
       "      <span class=\"abstract-short has-text-grey-dark mathjax\" id=\"2006.12150v2-abstract-short\" style=\"display: inline;\">\n",
       "        …images with multiple objects without explicitly requiring their contextual information during the generation process. Based on a vector-quantized variational autoencoder (<span class=\"search-hit mathjax\">VQ</span>-…\n",
       "        <a class=\"is-size-7\" onclick=\"document.getElementById('2006.12150v2-abstract-full').style.display = 'inline'; document.getElementById('2006.12150v2-abstract-short').style.display = 'none';\" style=\"white-space: nowrap;\">▽ More</a>\n",
       "</span>\n",
       "<span class=\"abstract-full has-text-grey-dark mathjax\" id=\"2006.12150v2-abstract-full\" style=\"display: none;\">\n",
       "        Recent developments related to generative models have made it possible to generate diverse high-fidelity images. In particular, layout-to-image generation models have gained significant attention due to their capability to generate realistic complex images containing distinct objects. These models are generally conditioned on either semantic layouts or textual descriptions. However, unlike natural images, providing auxiliary information can be extremely hard in domains such as biomedical imaging and remote sensing. In this work, we propose a multi-object generation framework that can synthesize images with multiple objects without explicitly requiring their contextual information during the generation process. Based on a vector-quantized variational autoencoder (<span class=\"search-hit mathjax\">VQ</span>-<span class=\"search-hit mathjax\">VAE</span>) backbone, our model learns to preserve spatial coherency within an image as well as semantic coherency between the objects and the background through two powerful autoregressive priors: PixelSNAIL and LayoutPixelSNAIL. While the PixelSNAIL learns the distribution of the latent encodings of the <span class=\"search-hit mathjax\">VQ</span>-<span class=\"search-hit mathjax\">VAE</span>, the LayoutPixelSNAIL is used to specifically learn the semantic distribution of the objects. An implicit advantage of our approach is that the generated samples are accompanied by object-level annotations. We demonstrate how coherency and fidelity are preserved with our method through experiments on the Multi-MNIST and CLEVR datasets; thereby outperforming state-of-the-art multi-object generative methods. The efficacy of our approach is demonstrated through application on medical imaging datasets, where we show that augmenting the training set with generated samples using our approach improves the performance of existing models.\n",
       "        <a class=\"is-size-7\" onclick=\"document.getElementById('2006.12150v2-abstract-full').style.display = 'none'; document.getElementById('2006.12150v2-abstract-short').style.display = 'inline';\" style=\"white-space: nowrap;\">△ Less</a>\n",
       "</span>\n",
       "</p>\n",
       "<p class=\"is-size-7\"><span class=\"has-text-black-bis has-text-weight-semibold\">Submitted</span> 24 June, 2020; <span class=\"has-text-black-bis has-text-weight-semibold\">v1</span> submitted 22 June, 2020;\n",
       "      <span class=\"has-text-black-bis has-text-weight-semibold\">originally announced</span> June 2020.\n",
       "      \n",
       "    </p>\n",
       "<p class=\"comments is-size-7\">\n",
       "<span class=\"has-text-black-bis has-text-weight-semibold\">Comments:</span>\n",
       "<span class=\"has-text-grey-dark mathjax\">21 pages, 5 tables, 21 figures</span>\n",
       "</p>\n",
       "</li>\n",
       "<li class=\"arxiv-result\">\n",
       "<div class=\"is-marginless\">\n",
       "<p class=\"list-title is-inline-block\"><a href=\"https://arxiv.org/abs/2006.07926\">arXiv:2006.07926</a>\n",
       "<span> [<a href=\"https://arxiv.org/pdf/2006.07926\">pdf</a>, <a href=\"https://arxiv.org/format/2006.07926\">other</a>] </span>\n",
       "</p>\n",
       "<div class=\"tags is-inline-block\">\n",
       "<span class=\"tag is-small is-link tooltip is-tooltip-top\" data-tooltip=\"Audio and Speech Processing\">eess.AS</span>\n",
       "<span class=\"tag is-small is-grey tooltip is-tooltip-top\" data-tooltip=\"Computation and Language\">cs.CL</span>\n",
       "</div>\n",
       "</div>\n",
       "<p class=\"title is-5 mathjax\">\n",
       "      \n",
       "        UWSpeech: Speech to Speech Translation for Unwritten Languages\n",
       "      \n",
       "    </p>\n",
       "<p class=\"authors\">\n",
       "<span class=\"has-text-black-bis has-text-weight-semibold\">Authors:</span>\n",
       "<a href=\"/search/?searchtype=author&amp;query=Zhang%2C+C\">Chen Zhang</a>, \n",
       "      \n",
       "      <a href=\"/search/?searchtype=author&amp;query=Tan%2C+X\">Xu Tan</a>, \n",
       "      \n",
       "      <a href=\"/search/?searchtype=author&amp;query=Ren%2C+Y\">Yi Ren</a>, \n",
       "      \n",
       "      <a href=\"/search/?searchtype=author&amp;query=Qin%2C+T\">Tao Qin</a>, \n",
       "      \n",
       "      <a href=\"/search/?searchtype=author&amp;query=Zhang%2C+K\">Kejun Zhang</a>, \n",
       "      \n",
       "      <a href=\"/search/?searchtype=author&amp;query=Liu%2C+T\">Tie-Yan Liu</a>\n",
       "</p>\n",
       "<p class=\"abstract mathjax\">\n",
       "<span class=\"search-hit\">Abstract</span>:\n",
       "      <span class=\"abstract-short has-text-grey-dark mathjax\" id=\"2006.07926v1-abstract-short\" style=\"display: inline;\">\n",
       "        …speech into target discrete tokens with a translator, and finally synthesizes target speech from target discrete tokens with an inverter. We propose a method called XL-<span class=\"search-hit mathjax\">VAE</span>, which enhances vector quantized variational autoencoder (…\n",
       "        <a class=\"is-size-7\" onclick=\"document.getElementById('2006.07926v1-abstract-full').style.display = 'inline'; document.getElementById('2006.07926v1-abstract-short').style.display = 'none';\" style=\"white-space: nowrap;\">▽ More</a>\n",
       "</span>\n",
       "<span class=\"abstract-full has-text-grey-dark mathjax\" id=\"2006.07926v1-abstract-full\" style=\"display: none;\">\n",
       "        Existing speech to speech translation systems heavily rely on the text of target language: they usually translate source language either to target text and then synthesize target speech from text, or directly to target speech with target text for auxiliary training. However, those methods cannot be applied to unwritten target languages, which have no written text or phoneme available. In this paper, we develop a translation system for unwritten languages, named as UWSpeech, which converts target unwritten speech into discrete tokens with a converter, and then translates source-language speech into target discrete tokens with a translator, and finally synthesizes target speech from target discrete tokens with an inverter. We propose a method called XL-<span class=\"search-hit mathjax\">VAE</span>, which enhances vector quantized variational autoencoder (<span class=\"search-hit mathjax\">VQ</span>-<span class=\"search-hit mathjax\">VAE</span>) with cross-lingual (XL) speech recognition, to train the converter and inverter of UWSpeech jointly. Experiments on Fisher Spanish-English conversation translation dataset show that UWSpeech outperforms direct translation and <span class=\"search-hit mathjax\">VQ</span>-<span class=\"search-hit mathjax\">VAE</span> baseline by about 16 and 10 BLEU points respectively, which demonstrate the advantages and potentials of UWSpeech.\n",
       "        <a class=\"is-size-7\" onclick=\"document.getElementById('2006.07926v1-abstract-full').style.display = 'none'; document.getElementById('2006.07926v1-abstract-short').style.display = 'inline';\" style=\"white-space: nowrap;\">△ Less</a>\n",
       "</span>\n",
       "</p>\n",
       "<p class=\"is-size-7\"><span class=\"has-text-black-bis has-text-weight-semibold\">Submitted</span> 14 June, 2020; \n",
       "      <span class=\"has-text-black-bis has-text-weight-semibold\">originally announced</span> June 2020.\n",
       "      \n",
       "    </p>\n",
       "</li>\n",
       "<li class=\"arxiv-result\">\n",
       "<div class=\"is-marginless\">\n",
       "<p class=\"list-title is-inline-block\"><a href=\"https://arxiv.org/abs/2005.11676\">arXiv:2005.11676</a>\n",
       "<span> [<a href=\"https://arxiv.org/pdf/2005.11676\">pdf</a>, <a href=\"https://arxiv.org/format/2005.11676\">other</a>] </span>\n",
       "</p>\n",
       "<div class=\"tags is-inline-block\">\n",
       "<span class=\"tag is-small is-link tooltip is-tooltip-top\" data-tooltip=\"Computation and Language\">cs.CL</span>\n",
       "<span class=\"tag is-small is-grey tooltip is-tooltip-top\" data-tooltip=\"Machine Learning\">cs.LG</span>\n",
       "<span class=\"tag is-small is-grey tooltip is-tooltip-top\" data-tooltip=\"Sound\">cs.SD</span>\n",
       "<span class=\"tag is-small is-grey tooltip is-tooltip-top\" data-tooltip=\"Audio and Speech Processing\">eess.AS</span>\n",
       "</div>\n",
       "</div>\n",
       "<p class=\"title is-5 mathjax\">\n",
       "      \n",
       "        Transformer <span class=\"search-hit mathjax\">VQ</span>-<span class=\"search-hit mathjax\">VAE</span> for Unsupervised Unit Discovery and Speech Synthesis: ZeroSpeech 2020 Challenge\n",
       "      \n",
       "    </p>\n",
       "<p class=\"authors\">\n",
       "<span class=\"has-text-black-bis has-text-weight-semibold\">Authors:</span>\n",
       "<a href=\"/search/?searchtype=author&amp;query=Tjandra%2C+A\">Andros Tjandra</a>, \n",
       "      \n",
       "      <a href=\"/search/?searchtype=author&amp;query=Sakti%2C+S\">Sakriani Sakti</a>, \n",
       "      \n",
       "      <a href=\"/search/?searchtype=author&amp;query=Nakamura%2C+S\">Satoshi Nakamura</a>\n",
       "</p>\n",
       "<p class=\"abstract mathjax\">\n",
       "<span class=\"search-hit\">Abstract</span>:\n",
       "      <span class=\"abstract-short has-text-grey-dark mathjax\" id=\"2005.11676v1-abstract-short\" style=\"display: inline;\">\n",
       "        …The system also needs to balance the codebook performance between the ABX error rate and the bitrate compression rate. Our main contribution here is we proposed Transformer-based <span class=\"search-hit mathjax\">VQ</span>-<span class=\"search-hit mathjax\">VAE</span> for unsupervised unit discovery and Transformer-based inverter for the speech synthesis given the extracted codebook. Additionally, we…\n",
       "        <a class=\"is-size-7\" onclick=\"document.getElementById('2005.11676v1-abstract-full').style.display = 'inline'; document.getElementById('2005.11676v1-abstract-short').style.display = 'none';\" style=\"white-space: nowrap;\">▽ More</a>\n",
       "</span>\n",
       "<span class=\"abstract-full has-text-grey-dark mathjax\" id=\"2005.11676v1-abstract-full\" style=\"display: none;\">\n",
       "        In this paper, we report our submitted system for the ZeroSpeech 2020 challenge on Track 2019. The main theme in this challenge is to build a speech synthesizer without any textual information or phonetic labels. In order to tackle those challenges, we build a system that must address two major components such as 1) given speech audio, extract subword units in an unsupervised way and 2) re-synthesize the audio from novel speakers. The system also needs to balance the codebook performance between the ABX error rate and the bitrate compression rate. Our main contribution here is we proposed Transformer-based <span class=\"search-hit mathjax\">VQ</span>-<span class=\"search-hit mathjax\">VAE</span> for unsupervised unit discovery and Transformer-based inverter for the speech synthesis given the extracted codebook. Additionally, we also explored several regularization methods to improve performance even further.\n",
       "        <a class=\"is-size-7\" onclick=\"document.getElementById('2005.11676v1-abstract-full').style.display = 'none'; document.getElementById('2005.11676v1-abstract-short').style.display = 'inline';\" style=\"white-space: nowrap;\">△ Less</a>\n",
       "</span>\n",
       "</p>\n",
       "<p class=\"is-size-7\"><span class=\"has-text-black-bis has-text-weight-semibold\">Submitted</span> 24 May, 2020; \n",
       "      <span class=\"has-text-black-bis has-text-weight-semibold\">originally announced</span> May 2020.\n",
       "      \n",
       "    </p>\n",
       "<p class=\"comments is-size-7\">\n",
       "<span class=\"has-text-black-bis has-text-weight-semibold\">Comments:</span>\n",
       "<span class=\"has-text-grey-dark mathjax\">Submitted to INTERSPEECH 2020</span>\n",
       "</p>\n",
       "</li>\n",
       "<li class=\"arxiv-result\">\n",
       "<div class=\"is-marginless\">\n",
       "<p class=\"list-title is-inline-block\"><a href=\"https://arxiv.org/abs/2005.09409\">arXiv:2005.09409</a>\n",
       "<span> [<a href=\"https://arxiv.org/pdf/2005.09409\">pdf</a>, <a href=\"https://arxiv.org/format/2005.09409\">other</a>] </span>\n",
       "</p>\n",
       "<div class=\"tags is-inline-block\">\n",
       "<span class=\"tag is-small is-link tooltip is-tooltip-top\" data-tooltip=\"Audio and Speech Processing\">eess.AS</span>\n",
       "<span class=\"tag is-small is-grey tooltip is-tooltip-top\" data-tooltip=\"Computation and Language\">cs.CL</span>\n",
       "</div>\n",
       "</div>\n",
       "<p class=\"title is-5 mathjax\">\n",
       "      \n",
       "        Vector-quantized neural networks for acoustic unit discovery in the ZeroSpeech 2020 challenge\n",
       "      \n",
       "    </p>\n",
       "<p class=\"authors\">\n",
       "<span class=\"has-text-black-bis has-text-weight-semibold\">Authors:</span>\n",
       "<a href=\"/search/?searchtype=author&amp;query=van+Niekerk%2C+B\">Benjamin van Niekerk</a>, \n",
       "      \n",
       "      <a href=\"/search/?searchtype=author&amp;query=Nortje%2C+L\">Leanne Nortje</a>, \n",
       "      \n",
       "      <a href=\"/search/?searchtype=author&amp;query=Kamper%2C+H\">Herman Kamper</a>\n",
       "</p>\n",
       "<p class=\"abstract mathjax\">\n",
       "<span class=\"search-hit\">Abstract</span>:\n",
       "      <span class=\"abstract-short has-text-grey-dark mathjax\" id=\"2005.09409v2-abstract-short\" style=\"display: inline;\">\n",
       "        …tackle this challenge - both use vector quantization to map continuous features to a finite set of codes. The first model is a type of vector-quantized variational autoencoder (<span class=\"search-hit mathjax\">VQ</span>-…\n",
       "        <a class=\"is-size-7\" onclick=\"document.getElementById('2005.09409v2-abstract-full').style.display = 'inline'; document.getElementById('2005.09409v2-abstract-short').style.display = 'none';\" style=\"white-space: nowrap;\">▽ More</a>\n",
       "</span>\n",
       "<span class=\"abstract-full has-text-grey-dark mathjax\" id=\"2005.09409v2-abstract-full\" style=\"display: none;\">\n",
       "        In this paper, we explore vector quantization for acoustic unit discovery. Leveraging unlabelled data, we aim to learn discrete representations of speech that separate phonetic content from speaker-specific details. We propose two neural models to tackle this challenge - both use vector quantization to map continuous features to a finite set of codes. The first model is a type of vector-quantized variational autoencoder (<span class=\"search-hit mathjax\">VQ</span>-<span class=\"search-hit mathjax\">VAE</span>). The <span class=\"search-hit mathjax\">VQ</span>-<span class=\"search-hit mathjax\">VAE</span> encodes speech into a sequence of discrete units before reconstructing the audio waveform. Our second model combines vector quantization with contrastive predictive coding (<span class=\"search-hit mathjax\">VQ</span>-CPC). The idea is to learn a representation of speech by predicting future acoustic units. We evaluate the models on English and Indonesian data for the ZeroSpeech 2020 challenge. In ABX phone discrimination tests, both models outperform all submissions to the 2019 and 2020 challenges, with a relative improvement of more than 30%. The models also perform competitively on a downstream voice conversion task. Of the two, <span class=\"search-hit mathjax\">VQ</span>-CPC performs slightly better in general and is simpler and faster to train. Finally, probing experiments show that vector quantization is an effective bottleneck, forcing the models to discard speaker information.\n",
       "        <a class=\"is-size-7\" onclick=\"document.getElementById('2005.09409v2-abstract-full').style.display = 'none'; document.getElementById('2005.09409v2-abstract-short').style.display = 'inline';\" style=\"white-space: nowrap;\">△ Less</a>\n",
       "</span>\n",
       "</p>\n",
       "<p class=\"is-size-7\"><span class=\"has-text-black-bis has-text-weight-semibold\">Submitted</span> 19 August, 2020; <span class=\"has-text-black-bis has-text-weight-semibold\">v1</span> submitted 19 May, 2020;\n",
       "      <span class=\"has-text-black-bis has-text-weight-semibold\">originally announced</span> May 2020.\n",
       "      \n",
       "    </p>\n",
       "<p class=\"comments is-size-7\">\n",
       "<span class=\"has-text-black-bis has-text-weight-semibold\">Comments:</span>\n",
       "<span class=\"has-text-grey-dark mathjax\">5 pages, 3 figures, 2 tables, accepted to Interspeech 2020</span>\n",
       "</p>\n",
       "</li>\n",
       "<li class=\"arxiv-result\">\n",
       "<div class=\"is-marginless\">\n",
       "<p class=\"list-title is-inline-block\"><a href=\"https://arxiv.org/abs/2005.08520\">arXiv:2005.08520</a>\n",
       "<span> [<a href=\"https://arxiv.org/pdf/2005.08520\">pdf</a>, <a href=\"https://arxiv.org/format/2005.08520\">other</a>] </span>\n",
       "</p>\n",
       "<div class=\"tags is-inline-block\">\n",
       "<span class=\"tag is-small is-link tooltip is-tooltip-top\" data-tooltip=\"Machine Learning\">cs.LG</span>\n",
       "<span class=\"tag is-small is-grey tooltip is-tooltip-top\" data-tooltip=\"Computation and Language\">cs.CL</span>\n",
       "<span class=\"tag is-small is-grey tooltip is-tooltip-top\" data-tooltip=\"Machine Learning\">stat.ML</span>\n",
       "</div>\n",
       "</div>\n",
       "<p class=\"title is-5 mathjax\">\n",
       "      \n",
       "        Robust Training of Vector Quantized Bottleneck Models\n",
       "      \n",
       "    </p>\n",
       "<p class=\"authors\">\n",
       "<span class=\"has-text-black-bis has-text-weight-semibold\">Authors:</span>\n",
       "<a href=\"/search/?searchtype=author&amp;query=%C5%81a%C5%84cucki%2C+A\">Adrian Łańcucki</a>, \n",
       "      \n",
       "      <a href=\"/search/?searchtype=author&amp;query=Chorowski%2C+J\">Jan Chorowski</a>, \n",
       "      \n",
       "      <a href=\"/search/?searchtype=author&amp;query=Sanchez%2C+G\">Guillaume Sanchez</a>, \n",
       "      \n",
       "      <a href=\"/search/?searchtype=author&amp;query=Marxer%2C+R\">Ricard Marxer</a>, \n",
       "      \n",
       "      <a href=\"/search/?searchtype=author&amp;query=Chen%2C+N\">Nanxin Chen</a>, \n",
       "      \n",
       "      <a href=\"/search/?searchtype=author&amp;query=Dolfing%2C+H+J+G+A\">Hans J. G. A. Dolfing</a>, \n",
       "      \n",
       "      <a href=\"/search/?searchtype=author&amp;query=Khurana%2C+S\">Sameer Khurana</a>, \n",
       "      \n",
       "      <a href=\"/search/?searchtype=author&amp;query=Alum%C3%A4e%2C+T\">Tanel Alumäe</a>, \n",
       "      \n",
       "      <a href=\"/search/?searchtype=author&amp;query=Laurent%2C+A\">Antoine Laurent</a>\n",
       "</p>\n",
       "<p class=\"abstract mathjax\">\n",
       "<span class=\"search-hit\">Abstract</span>:\n",
       "      <span class=\"abstract-short has-text-grey-dark mathjax\" id=\"2005.08520v1-abstract-short\" style=\"display: inline;\">\n",
       "        In this paper we demonstrate methods for reliable and efficient training of discrete representation using Vector-Quantized Variational Auto-Encoder models (<span class=\"search-hit mathjax\">VQ</span>-…\n",
       "        <a class=\"is-size-7\" onclick=\"document.getElementById('2005.08520v1-abstract-full').style.display = 'inline'; document.getElementById('2005.08520v1-abstract-short').style.display = 'none';\" style=\"white-space: nowrap;\">▽ More</a>\n",
       "</span>\n",
       "<span class=\"abstract-full has-text-grey-dark mathjax\" id=\"2005.08520v1-abstract-full\" style=\"display: none;\">\n",
       "        In this paper we demonstrate methods for reliable and efficient training of discrete representation using Vector-Quantized Variational Auto-Encoder models (<span class=\"search-hit mathjax\">VQ</span>-<span class=\"search-hit mathjax\">VAEs</span>). Discrete latent variable models have been shown to learn nontrivial representations of speech, applicable to unsupervised voice conversion and reaching state-of-the-art performance on unit discovery tasks. For unsupervised representation learning, they became viable alternatives to continuous latent variable models such as the Variational Auto-Encoder (<span class=\"search-hit mathjax\">VAE</span>). However, training deep discrete variable models is challenging, due to the inherent non-differentiability of the discretization operation. In this paper we focus on <span class=\"search-hit mathjax\">VQ</span>-<span class=\"search-hit mathjax\">VAE</span>, a state-of-the-art discrete bottleneck model shown to perform on par with its continuous counterparts. It quantizes encoder outputs with on-line $k$-means clustering. We show that the codebook learning can suffer from poor initialization and non-stationarity of clustered encoder outputs. We demonstrate that these can be successfully overcome by increasing the learning rate for the codebook and periodic date-dependent codeword re-initialization. As a result, we achieve more robust training across different tasks, and significantly increase the usage of latent codewords even for large codebooks. This has practical benefit, for instance, in unsupervised representation learning, where large codebooks may lead to disentanglement of latent representations.\n",
       "        <a class=\"is-size-7\" onclick=\"document.getElementById('2005.08520v1-abstract-full').style.display = 'none'; document.getElementById('2005.08520v1-abstract-short').style.display = 'inline';\" style=\"white-space: nowrap;\">△ Less</a>\n",
       "</span>\n",
       "</p>\n",
       "<p class=\"is-size-7\"><span class=\"has-text-black-bis has-text-weight-semibold\">Submitted</span> 18 May, 2020; \n",
       "      <span class=\"has-text-black-bis has-text-weight-semibold\">originally announced</span> May 2020.\n",
       "      \n",
       "    </p>\n",
       "<p class=\"comments is-size-7\">\n",
       "<span class=\"has-text-black-bis has-text-weight-semibold\">Comments:</span>\n",
       "<span class=\"has-text-grey-dark mathjax\">Published at IJCNN 2020</span>\n",
       "</p>\n",
       "</li>\n",
       "<li class=\"arxiv-result\">\n",
       "<div class=\"is-marginless\">\n",
       "<p class=\"list-title is-inline-block\"><a href=\"https://arxiv.org/abs/2005.07884\">arXiv:2005.07884</a>\n",
       "<span> [<a href=\"https://arxiv.org/pdf/2005.07884\">pdf</a>, <a href=\"https://arxiv.org/format/2005.07884\">other</a>] </span>\n",
       "</p>\n",
       "<div class=\"tags is-inline-block\">\n",
       "<span class=\"tag is-small is-link tooltip is-tooltip-top\" data-tooltip=\"Audio and Speech Processing\">eess.AS</span>\n",
       "<span class=\"tag is-small is-grey tooltip is-tooltip-top\" data-tooltip=\"Sound\">cs.SD</span>\n",
       "</div>\n",
       "</div>\n",
       "<p class=\"title is-5 mathjax\">\n",
       "      \n",
       "        Improved Prosody from Learned F0 Codebook Representations for <span class=\"search-hit mathjax\">VQ</span>-<span class=\"search-hit mathjax\">VAE</span> Speech Waveform Reconstruction\n",
       "      \n",
       "    </p>\n",
       "<p class=\"authors\">\n",
       "<span class=\"has-text-black-bis has-text-weight-semibold\">Authors:</span>\n",
       "<a href=\"/search/?searchtype=author&amp;query=Zhao%2C+Y\">Yi Zhao</a>, \n",
       "      \n",
       "      <a href=\"/search/?searchtype=author&amp;query=Li%2C+H\">Haoyu Li</a>, \n",
       "      \n",
       "      <a href=\"/search/?searchtype=author&amp;query=Lai%2C+C\">Cheng-I Lai</a>, \n",
       "      \n",
       "      <a href=\"/search/?searchtype=author&amp;query=Williams%2C+J\">Jennifer Williams</a>, \n",
       "      \n",
       "      <a href=\"/search/?searchtype=author&amp;query=Cooper%2C+E\">Erica Cooper</a>, \n",
       "      \n",
       "      <a href=\"/search/?searchtype=author&amp;query=Yamagishi%2C+J\">Junichi Yamagishi</a>\n",
       "</p>\n",
       "<p class=\"abstract mathjax\">\n",
       "<span class=\"search-hit\">Abstract</span>:\n",
       "      <span class=\"abstract-short has-text-grey-dark mathjax\" id=\"2005.07884v1-abstract-short\" style=\"display: inline;\">\n",
       "        Vector Quantized Variational AutoEncoders (<span class=\"search-hit mathjax\">VQ</span>-…\n",
       "        <a class=\"is-size-7\" onclick=\"document.getElementById('2005.07884v1-abstract-full').style.display = 'inline'; document.getElementById('2005.07884v1-abstract-short').style.display = 'none';\" style=\"white-space: nowrap;\">▽ More</a>\n",
       "</span>\n",
       "<span class=\"abstract-full has-text-grey-dark mathjax\" id=\"2005.07884v1-abstract-full\" style=\"display: none;\">\n",
       "        Vector Quantized Variational AutoEncoders (<span class=\"search-hit mathjax\">VQ</span>-<span class=\"search-hit mathjax\">VAE</span>) are a powerful representation learning framework that can discover discrete groups of features from a speech signal without supervision. Until now, the <span class=\"search-hit mathjax\">VQ</span>-<span class=\"search-hit mathjax\">VAE</span> architecture has previously modeled individual types of speech features, such as only phones or only F0. This paper introduces an important extension to <span class=\"search-hit mathjax\">VQ</span>-<span class=\"search-hit mathjax\">VAE</span> for learning F0-related suprasegmental information simultaneously along with traditional phone features.The proposed framework uses two encoders such that the F0 trajectory and speech waveform are both input to the system, therefore two separate codebooks are learned. We used a WaveRNN vocoder as the decoder component of <span class=\"search-hit mathjax\">VQ</span>-<span class=\"search-hit mathjax\">VAE</span>. Our speaker-independent <span class=\"search-hit mathjax\">VQ</span>-<span class=\"search-hit mathjax\">VAE</span> was trained with raw speech waveforms from multi-speaker Japanese speech databases. Experimental results show that the proposed extension reduces F0 distortion of reconstructed speech for all unseen test speakers, and results in significantly higher preference scores from a listening test. We additionally conducted experiments using single-speaker Mandarin speech to demonstrate advantages of our architecture in another language which relies heavily on F0.\n",
       "        <a class=\"is-size-7\" onclick=\"document.getElementById('2005.07884v1-abstract-full').style.display = 'none'; document.getElementById('2005.07884v1-abstract-short').style.display = 'inline';\" style=\"white-space: nowrap;\">△ Less</a>\n",
       "</span>\n",
       "</p>\n",
       "<p class=\"is-size-7\"><span class=\"has-text-black-bis has-text-weight-semibold\">Submitted</span> 16 May, 2020; \n",
       "      <span class=\"has-text-black-bis has-text-weight-semibold\">originally announced</span> May 2020.\n",
       "      \n",
       "    </p>\n",
       "<p class=\"comments is-size-7\">\n",
       "<span class=\"has-text-black-bis has-text-weight-semibold\">Comments:</span>\n",
       "<span class=\"has-text-grey-dark mathjax\">Submitted to Interspeech 2020</span>\n",
       "</p>\n",
       "</li>\n",
       "<li class=\"arxiv-result\">\n",
       "<div class=\"is-marginless\">\n",
       "<p class=\"list-title is-inline-block\"><a href=\"https://arxiv.org/abs/2005.05525\">arXiv:2005.05525</a>\n",
       "<span> [<a href=\"https://arxiv.org/pdf/2005.05525\">pdf</a>, <a href=\"https://arxiv.org/format/2005.05525\">other</a>] </span>\n",
       "</p>\n",
       "<div class=\"tags is-inline-block\">\n",
       "<span class=\"tag is-small is-link tooltip is-tooltip-top\" data-tooltip=\"Computation and Language\">cs.CL</span>\n",
       "<span class=\"tag is-small is-grey tooltip is-tooltip-top\" data-tooltip=\"Sound\">cs.SD</span>\n",
       "<span class=\"tag is-small is-grey tooltip is-tooltip-top\" data-tooltip=\"Audio and Speech Processing\">eess.AS</span>\n",
       "</div>\n",
       "</div>\n",
       "<p class=\"title is-5 mathjax\">\n",
       "      \n",
       "        DiscreTalk: Text-to-Speech as a Machine Translation Problem\n",
       "      \n",
       "    </p>\n",
       "<p class=\"authors\">\n",
       "<span class=\"has-text-black-bis has-text-weight-semibold\">Authors:</span>\n",
       "<a href=\"/search/?searchtype=author&amp;query=Hayashi%2C+T\">Tomoki Hayashi</a>, \n",
       "      \n",
       "      <a href=\"/search/?searchtype=author&amp;query=Watanabe%2C+S\">Shinji Watanabe</a>\n",
       "</p>\n",
       "<p class=\"abstract mathjax\">\n",
       "<span class=\"search-hit\">Abstract</span>:\n",
       "      <span class=\"abstract-short has-text-grey-dark mathjax\" id=\"2005.05525v1-abstract-short\" style=\"display: inline;\">\n",
       "        …(E2E-TTS) model based on neural machine translation (NMT). The proposed model consists of two components; a non-autoregressive vector quantized variational autoencoder (<span class=\"search-hit mathjax\">VQ</span>-…\n",
       "        <a class=\"is-size-7\" onclick=\"document.getElementById('2005.05525v1-abstract-full').style.display = 'inline'; document.getElementById('2005.05525v1-abstract-short').style.display = 'none';\" style=\"white-space: nowrap;\">▽ More</a>\n",
       "</span>\n",
       "<span class=\"abstract-full has-text-grey-dark mathjax\" id=\"2005.05525v1-abstract-full\" style=\"display: none;\">\n",
       "        This paper proposes a new end-to-end text-to-speech (E2E-TTS) model based on neural machine translation (NMT). The proposed model consists of two components; a non-autoregressive vector quantized variational autoencoder (<span class=\"search-hit mathjax\">VQ</span>-<span class=\"search-hit mathjax\">VAE</span>) model and an autoregressive Transformer-NMT model. The <span class=\"search-hit mathjax\">VQ</span>-<span class=\"search-hit mathjax\">VAE</span> model learns a mapping function from a speech waveform into a sequence of discrete symbols, and then the Transformer-NMT model is trained to estimate this discrete symbol sequence from a given input text. Since the <span class=\"search-hit mathjax\">VQ</span>-<span class=\"search-hit mathjax\">VAE</span> model can learn such a mapping in a fully-data-driven manner, we do not need to consider hyperparameters of the feature extraction required in the conventional E2E-TTS models. Thanks to the use of discrete symbols, we can use various techniques developed in NMT and automatic speech recognition (ASR) such as beam search, subword units, and fusions with a language model. Furthermore, we can avoid an over smoothing problem of predicted features, which is one of the common issues in TTS. The experimental evaluation with the JSUT corpus shows that the proposed method outperforms the conventional Transformer-TTS model with a non-autoregressive neural vocoder in naturalness, achieving the performance comparable to the reconstruction of the <span class=\"search-hit mathjax\">VQ</span>-<span class=\"search-hit mathjax\">VAE</span> model.\n",
       "        <a class=\"is-size-7\" onclick=\"document.getElementById('2005.05525v1-abstract-full').style.display = 'none'; document.getElementById('2005.05525v1-abstract-short').style.display = 'inline';\" style=\"white-space: nowrap;\">△ Less</a>\n",
       "</span>\n",
       "</p>\n",
       "<p class=\"is-size-7\"><span class=\"has-text-black-bis has-text-weight-semibold\">Submitted</span> 11 May, 2020; \n",
       "      <span class=\"has-text-black-bis has-text-weight-semibold\">originally announced</span> May 2020.\n",
       "      \n",
       "    </p>\n",
       "<p class=\"comments is-size-7\">\n",
       "<span class=\"has-text-black-bis has-text-weight-semibold\">Comments:</span>\n",
       "<span class=\"has-text-grey-dark mathjax\">Submitted to INTERSPEECH 2020. The demo is available on https://kan-bayashi.github.io/DiscreTalk/</span>\n",
       "</p>\n",
       "</li>\n",
       "<li class=\"arxiv-result\">\n",
       "<div class=\"is-marginless\">\n",
       "<p class=\"list-title is-inline-block\"><a href=\"https://arxiv.org/abs/2005.00341\">arXiv:2005.00341</a>\n",
       "<span> [<a href=\"https://arxiv.org/pdf/2005.00341\">pdf</a>, <a href=\"https://arxiv.org/format/2005.00341\">other</a>] </span>\n",
       "</p>\n",
       "<div class=\"tags is-inline-block\">\n",
       "<span class=\"tag is-small is-link tooltip is-tooltip-top\" data-tooltip=\"Audio and Speech Processing\">eess.AS</span>\n",
       "<span class=\"tag is-small is-grey tooltip is-tooltip-top\" data-tooltip=\"Machine Learning\">cs.LG</span>\n",
       "<span class=\"tag is-small is-grey tooltip is-tooltip-top\" data-tooltip=\"Sound\">cs.SD</span>\n",
       "<span class=\"tag is-small is-grey tooltip is-tooltip-top\" data-tooltip=\"Machine Learning\">stat.ML</span>\n",
       "</div>\n",
       "</div>\n",
       "<p class=\"title is-5 mathjax\">\n",
       "      \n",
       "        Jukebox: A Generative Model for Music\n",
       "      \n",
       "    </p>\n",
       "<p class=\"authors\">\n",
       "<span class=\"has-text-black-bis has-text-weight-semibold\">Authors:</span>\n",
       "<a href=\"/search/?searchtype=author&amp;query=Dhariwal%2C+P\">Prafulla Dhariwal</a>, \n",
       "      \n",
       "      <a href=\"/search/?searchtype=author&amp;query=Jun%2C+H\">Heewoo Jun</a>, \n",
       "      \n",
       "      <a href=\"/search/?searchtype=author&amp;query=Payne%2C+C\">Christine Payne</a>, \n",
       "      \n",
       "      <a href=\"/search/?searchtype=author&amp;query=Kim%2C+J+W\">Jong Wook Kim</a>, \n",
       "      \n",
       "      <a href=\"/search/?searchtype=author&amp;query=Radford%2C+A\">Alec Radford</a>, \n",
       "      \n",
       "      <a href=\"/search/?searchtype=author&amp;query=Sutskever%2C+I\">Ilya Sutskever</a>\n",
       "</p>\n",
       "<p class=\"abstract mathjax\">\n",
       "<span class=\"search-hit\">Abstract</span>:\n",
       "      <span class=\"abstract-short has-text-grey-dark mathjax\" id=\"2005.00341v1-abstract-short\" style=\"display: inline;\">\n",
       "        We introduce Jukebox, a model that generates music with singing in the raw audio domain. We tackle the long context of raw audio using a multi-scale <span class=\"search-hit mathjax\">VQ</span>-<span class=\"search-hit mathjax\">VAE</span> to compress it to discrete codes, and modeling those using autoregressive Transformers. We show that the combined model at scale can generate high-fidelity and dive…\n",
       "        <a class=\"is-size-7\" onclick=\"document.getElementById('2005.00341v1-abstract-full').style.display = 'inline'; document.getElementById('2005.00341v1-abstract-short').style.display = 'none';\" style=\"white-space: nowrap;\">▽ More</a>\n",
       "</span>\n",
       "<span class=\"abstract-full has-text-grey-dark mathjax\" id=\"2005.00341v1-abstract-full\" style=\"display: none;\">\n",
       "        We introduce Jukebox, a model that generates music with singing in the raw audio domain. We tackle the long context of raw audio using a multi-scale <span class=\"search-hit mathjax\">VQ</span>-<span class=\"search-hit mathjax\">VAE</span> to compress it to discrete codes, and modeling those using autoregressive Transformers. We show that the combined model at scale can generate high-fidelity and diverse songs with coherence up to multiple minutes. We can condition on artist and genre to steer the musical and vocal style, and on unaligned lyrics to make the singing more controllable. We are releasing thousands of non cherry-picked samples at https://jukebox.openai.com, along with model weights and code at https://github.com/openai/jukebox\n",
       "        <a class=\"is-size-7\" onclick=\"document.getElementById('2005.00341v1-abstract-full').style.display = 'none'; document.getElementById('2005.00341v1-abstract-short').style.display = 'inline';\" style=\"white-space: nowrap;\">△ Less</a>\n",
       "</span>\n",
       "</p>\n",
       "<p class=\"is-size-7\"><span class=\"has-text-black-bis has-text-weight-semibold\">Submitted</span> 30 April, 2020; \n",
       "      <span class=\"has-text-black-bis has-text-weight-semibold\">originally announced</span> May 2020.\n",
       "      \n",
       "    </p>\n",
       "</li>\n",
       "<li class=\"arxiv-result\">\n",
       "<div class=\"is-marginless\">\n",
       "<p class=\"list-title is-inline-block\"><a href=\"https://arxiv.org/abs/2003.01599\">arXiv:2003.01599</a>\n",
       "<span> [<a href=\"https://arxiv.org/pdf/2003.01599\">pdf</a>, <a href=\"https://arxiv.org/format/2003.01599\">other</a>] </span>\n",
       "</p>\n",
       "<div class=\"tags is-inline-block\">\n",
       "<span class=\"tag is-small is-link tooltip is-tooltip-top\" data-tooltip=\"Machine Learning\">cs.LG</span>\n",
       "<span class=\"tag is-small is-grey tooltip is-tooltip-top\" data-tooltip=\"Machine Learning\">stat.ML</span>\n",
       "</div>\n",
       "</div>\n",
       "<p class=\"title is-5 mathjax\">\n",
       "<span class=\"search-hit mathjax\">VQ</span>-DRAW: A Sequential Discrete <span class=\"search-hit mathjax\">VAE</span>\n",
       "</p>\n",
       "<p class=\"authors\">\n",
       "<span class=\"has-text-black-bis has-text-weight-semibold\">Authors:</span>\n",
       "<a href=\"/search/?searchtype=author&amp;query=Nichol%2C+A\">Alex Nichol</a>\n",
       "</p>\n",
       "<p class=\"abstract mathjax\">\n",
       "<span class=\"has-text-black-bis has-text-weight-semibold\">Abstract</span>:\n",
       "      <span class=\"abstract-short has-text-grey-dark mathjax\" id=\"2003.01599v1-abstract-short\" style=\"display: inline;\">\n",
       "        In this paper, I present <span class=\"search-hit mathjax\">VQ</span>-DRAW, an algorithm for learning compact discrete representations of data. <span class=\"search-hit mathjax\">VQ</span>-DRAW leverages a vector quantization effect to adapt the sequential generation scheme of DRAW to discrete latent variables. I show that <span class=\"search-hit mathjax\">VQ</span>-DRAW can effectively learn to compre…\n",
       "        <a class=\"is-size-7\" onclick=\"document.getElementById('2003.01599v1-abstract-full').style.display = 'inline'; document.getElementById('2003.01599v1-abstract-short').style.display = 'none';\" style=\"white-space: nowrap;\">▽ More</a>\n",
       "</span>\n",
       "<span class=\"abstract-full has-text-grey-dark mathjax\" id=\"2003.01599v1-abstract-full\" style=\"display: none;\">\n",
       "        In this paper, I present <span class=\"search-hit mathjax\">VQ</span>-DRAW, an algorithm for learning compact discrete representations of data. <span class=\"search-hit mathjax\">VQ</span>-DRAW leverages a vector quantization effect to adapt the sequential generation scheme of DRAW to discrete latent variables. I show that <span class=\"search-hit mathjax\">VQ</span>-DRAW can effectively learn to compress images from a variety of common datasets, as well as generate realistic samples from these datasets with no help from an autoregressive prior.\n",
       "        <a class=\"is-size-7\" onclick=\"document.getElementById('2003.01599v1-abstract-full').style.display = 'none'; document.getElementById('2003.01599v1-abstract-short').style.display = 'inline';\" style=\"white-space: nowrap;\">△ Less</a>\n",
       "</span>\n",
       "</p>\n",
       "<p class=\"is-size-7\"><span class=\"has-text-black-bis has-text-weight-semibold\">Submitted</span> 3 March, 2020; \n",
       "      <span class=\"has-text-black-bis has-text-weight-semibold\">originally announced</span> March 2020.\n",
       "      \n",
       "    </p>\n",
       "</li>\n",
       "<li class=\"arxiv-result\">\n",
       "<div class=\"is-marginless\">\n",
       "<p class=\"list-title is-inline-block\"><a href=\"https://arxiv.org/abs/2002.08111\">arXiv:2002.08111</a>\n",
       "<span> [<a href=\"https://arxiv.org/pdf/2002.08111\">pdf</a>, <a href=\"https://arxiv.org/format/2002.08111\">other</a>] </span>\n",
       "</p>\n",
       "<div class=\"tags is-inline-block\">\n",
       "<span class=\"tag is-small is-link tooltip is-tooltip-top\" data-tooltip=\"Machine Learning\">cs.LG</span>\n",
       "<span class=\"tag is-small is-grey tooltip is-tooltip-top\" data-tooltip=\"Computer Vision and Pattern Recognition\">cs.CV</span>\n",
       "<span class=\"tag is-small is-grey tooltip is-tooltip-top\" data-tooltip=\"Neural and Evolutionary Computing\">cs.NE</span>\n",
       "<span class=\"tag is-small is-grey tooltip is-tooltip-top\" data-tooltip=\"Machine Learning\">stat.ML</span>\n",
       "</div>\n",
       "</div>\n",
       "<p class=\"title is-5 mathjax\">\n",
       "      \n",
       "        Hierarchical Quantized Autoencoders\n",
       "      \n",
       "    </p>\n",
       "<p class=\"authors\">\n",
       "<span class=\"has-text-black-bis has-text-weight-semibold\">Authors:</span>\n",
       "<a href=\"/search/?searchtype=author&amp;query=Williams%2C+W\">Will Williams</a>, \n",
       "      \n",
       "      <a href=\"/search/?searchtype=author&amp;query=Ringer%2C+S\">Sam Ringer</a>, \n",
       "      \n",
       "      <a href=\"/search/?searchtype=author&amp;query=Ash%2C+T\">Tom Ash</a>, \n",
       "      \n",
       "      <a href=\"/search/?searchtype=author&amp;query=Hughes%2C+J\">John Hughes</a>, \n",
       "      \n",
       "      <a href=\"/search/?searchtype=author&amp;query=MacLeod%2C+D\">David MacLeod</a>, \n",
       "      \n",
       "      <a href=\"/search/?searchtype=author&amp;query=Dougherty%2C+J\">Jamie Dougherty</a>\n",
       "</p>\n",
       "<p class=\"abstract mathjax\">\n",
       "<span class=\"search-hit\">Abstract</span>:\n",
       "      <span class=\"abstract-short has-text-grey-dark mathjax\" id=\"2002.08111v2-abstract-short\" style=\"display: inline;\">\n",
       "        …perceptual quality and abstract features at very low bitrates. Encouraged by recent success in learning discrete representations with Vector Quantized Variational Autoencoders (<span class=\"search-hit mathjax\">VQ</span>-…\n",
       "        <a class=\"is-size-7\" onclick=\"document.getElementById('2002.08111v2-abstract-full').style.display = 'inline'; document.getElementById('2002.08111v2-abstract-short').style.display = 'none';\" style=\"white-space: nowrap;\">▽ More</a>\n",
       "</span>\n",
       "<span class=\"abstract-full has-text-grey-dark mathjax\" id=\"2002.08111v2-abstract-full\" style=\"display: none;\">\n",
       "        Despite progress in training neural networks for lossy image compression, current approaches fail to maintain both perceptual quality and abstract features at very low bitrates. Encouraged by recent success in learning discrete representations with Vector Quantized Variational Autoencoders (<span class=\"search-hit mathjax\">VQ</span>-<span class=\"search-hit mathjax\">VAEs</span>), we motivate the use of a hierarchy of <span class=\"search-hit mathjax\">VQ</span>-<span class=\"search-hit mathjax\">VAEs</span> to attain high factors of compression. We show that the combination of stochastic quantization and hierarchical latent structure aids likelihood-based image compression. This leads us to introduce a novel objective for training hierarchical <span class=\"search-hit mathjax\">VQ</span>-<span class=\"search-hit mathjax\">VAEs</span>. Our resulting scheme produces a Markovian series of latent variables that reconstruct images of high-perceptual quality which retain semantically meaningful features. We provide qualitative and quantitative evaluations on the CelebA and MNIST datasets.\n",
       "        <a class=\"is-size-7\" onclick=\"document.getElementById('2002.08111v2-abstract-full').style.display = 'none'; document.getElementById('2002.08111v2-abstract-short').style.display = 'inline';\" style=\"white-space: nowrap;\">△ Less</a>\n",
       "</span>\n",
       "</p>\n",
       "<p class=\"is-size-7\"><span class=\"has-text-black-bis has-text-weight-semibold\">Submitted</span> 4 June, 2020; <span class=\"has-text-black-bis has-text-weight-semibold\">v1</span> submitted 19 February, 2020;\n",
       "      <span class=\"has-text-black-bis has-text-weight-semibold\">originally announced</span> February 2020.\n",
       "      \n",
       "    </p>\n",
       "</li>\n",
       "<li class=\"arxiv-result\">\n",
       "<div class=\"is-marginless\">\n",
       "<p class=\"list-title is-inline-block\"><a href=\"https://arxiv.org/abs/2002.05692\">arXiv:2002.05692</a>\n",
       "<span> [<a href=\"https://arxiv.org/pdf/2002.05692\">pdf</a>, <a href=\"https://arxiv.org/format/2002.05692\">other</a>] </span>\n",
       "</p>\n",
       "<div class=\"tags is-inline-block\">\n",
       "<span class=\"tag is-small is-link tooltip is-tooltip-top\" data-tooltip=\"Image and Video Processing\">eess.IV</span>\n",
       "<span class=\"tag is-small is-grey tooltip is-tooltip-top\" data-tooltip=\"Computer Vision and Pattern Recognition\">cs.CV</span>\n",
       "<span class=\"tag is-small is-grey tooltip is-tooltip-top\" data-tooltip=\"Quantitative Methods\">q-bio.QM</span>\n",
       "</div>\n",
       "</div>\n",
       "<p class=\"title is-5 mathjax\">\n",
       "      \n",
       "        Neuromorphologicaly-preserving Volumetric data encoding using <span class=\"search-hit mathjax\">VQ</span>-<span class=\"search-hit mathjax\">VAE</span>\n",
       "</p>\n",
       "<p class=\"authors\">\n",
       "<span class=\"has-text-black-bis has-text-weight-semibold\">Authors:</span>\n",
       "<a href=\"/search/?searchtype=author&amp;query=Tudosiu%2C+P\">Petru-Daniel Tudosiu</a>, \n",
       "      \n",
       "      <a href=\"/search/?searchtype=author&amp;query=Varsavsky%2C+T\">Thomas Varsavsky</a>, \n",
       "      \n",
       "      <a href=\"/search/?searchtype=author&amp;query=Shaw%2C+R\">Richard Shaw</a>, \n",
       "      \n",
       "      <a href=\"/search/?searchtype=author&amp;query=Graham%2C+M\">Mark Graham</a>, \n",
       "      \n",
       "      <a href=\"/search/?searchtype=author&amp;query=Nachev%2C+P\">Parashkev Nachev</a>, \n",
       "      \n",
       "      <a href=\"/search/?searchtype=author&amp;query=Ourselin%2C+S\">Sebastien Ourselin</a>, \n",
       "      \n",
       "      <a href=\"/search/?searchtype=author&amp;query=Sudre%2C+C+H\">Carole H. Sudre</a>, \n",
       "      \n",
       "      <a href=\"/search/?searchtype=author&amp;query=Cardoso%2C+M+J\">M. Jorge Cardoso</a>\n",
       "</p>\n",
       "<p class=\"abstract mathjax\">\n",
       "<span class=\"search-hit\">Abstract</span>:\n",
       "      <span class=\"abstract-short has-text-grey-dark mathjax\" id=\"2002.05692v1-abstract-short\" style=\"display: inline;\">\n",
       "        …improvements, have enabled the complex and high-dimensional modelling of medical volumetric data at higher resolutions. Recently, Vector-Quantised Variational Autoencoders (<span class=\"search-hit mathjax\">VQ</span>-…\n",
       "        <a class=\"is-size-7\" onclick=\"document.getElementById('2002.05692v1-abstract-full').style.display = 'inline'; document.getElementById('2002.05692v1-abstract-short').style.display = 'none';\" style=\"white-space: nowrap;\">▽ More</a>\n",
       "</span>\n",
       "<span class=\"abstract-full has-text-grey-dark mathjax\" id=\"2002.05692v1-abstract-full\" style=\"display: none;\">\n",
       "        The increasing efficiency and compactness of deep learning architectures, together with hardware improvements, have enabled the complex and high-dimensional modelling of medical volumetric data at higher resolutions. Recently, Vector-Quantised Variational Autoencoders (<span class=\"search-hit mathjax\">VQ</span>-<span class=\"search-hit mathjax\">VAE</span>) have been proposed as an efficient generative unsupervised learning approach that can encode images to a small percentage of their initial size, while preserving their decoded fidelity. Here, we show a <span class=\"search-hit mathjax\">VQ</span>-<span class=\"search-hit mathjax\">VAE</span> inspired network can efficiently encode a full-resolution 3D brain volume, compressing the data to $0.825\\%$ of the original size while maintaining image fidelity, and significantly outperforming the previous state-of-the-art. We then demonstrate that <span class=\"search-hit mathjax\">VQ</span>-<span class=\"search-hit mathjax\">VAE</span> decoded images preserve the morphological characteristics of the original data through voxel-based morphology and segmentation experiments. Lastly, we show that such models can be pre-trained and then fine-tuned on different datasets without the introduction of bias.\n",
       "        <a class=\"is-size-7\" onclick=\"document.getElementById('2002.05692v1-abstract-full').style.display = 'none'; document.getElementById('2002.05692v1-abstract-short').style.display = 'inline';\" style=\"white-space: nowrap;\">△ Less</a>\n",
       "</span>\n",
       "</p>\n",
       "<p class=\"is-size-7\"><span class=\"has-text-black-bis has-text-weight-semibold\">Submitted</span> 13 February, 2020; \n",
       "      <span class=\"has-text-black-bis has-text-weight-semibold\">originally announced</span> February 2020.\n",
       "      \n",
       "    </p>\n",
       "</li>\n",
       "<li class=\"arxiv-result\">\n",
       "<div class=\"is-marginless\">\n",
       "<p class=\"list-title is-inline-block\"><a href=\"https://arxiv.org/abs/2002.03788\">arXiv:2002.03788</a>\n",
       "<span> [<a href=\"https://arxiv.org/pdf/2002.03788\">pdf</a>, <a href=\"https://arxiv.org/format/2002.03788\">other</a>] </span>\n",
       "</p>\n",
       "<div class=\"tags is-inline-block\">\n",
       "<span class=\"tag is-small is-link tooltip is-tooltip-top\" data-tooltip=\"Audio and Speech Processing\">eess.AS</span>\n",
       "<span class=\"tag is-small is-grey tooltip is-tooltip-top\" data-tooltip=\"Machine Learning\">cs.LG</span>\n",
       "<span class=\"tag is-small is-grey tooltip is-tooltip-top\" data-tooltip=\"Sound\">cs.SD</span>\n",
       "<span class=\"tag is-small is-grey tooltip is-tooltip-top\" data-tooltip=\"Machine Learning\">stat.ML</span>\n",
       "</div>\n",
       "</div>\n",
       "<p class=\"title is-5 mathjax\">\n",
       "      \n",
       "        Generating diverse and natural text-to-speech samples using a quantized fine-grained <span class=\"search-hit mathjax\">VAE</span> and auto-regressive prosody prior\n",
       "      \n",
       "    </p>\n",
       "<p class=\"authors\">\n",
       "<span class=\"has-text-black-bis has-text-weight-semibold\">Authors:</span>\n",
       "<a href=\"/search/?searchtype=author&amp;query=Sun%2C+G\">Guangzhi Sun</a>, \n",
       "      \n",
       "      <a href=\"/search/?searchtype=author&amp;query=Zhang%2C+Y\">Yu Zhang</a>, \n",
       "      \n",
       "      <a href=\"/search/?searchtype=author&amp;query=Weiss%2C+R+J\">Ron J. Weiss</a>, \n",
       "      \n",
       "      <a href=\"/search/?searchtype=author&amp;query=Cao%2C+Y\">Yuan Cao</a>, \n",
       "      \n",
       "      <a href=\"/search/?searchtype=author&amp;query=Zen%2C+H\">Heiga Zen</a>, \n",
       "      \n",
       "      <a href=\"/search/?searchtype=author&amp;query=Rosenberg%2C+A\">Andrew Rosenberg</a>, \n",
       "      \n",
       "      <a href=\"/search/?searchtype=author&amp;query=Ramabhadran%2C+B\">Bhuvana Ramabhadran</a>, \n",
       "      \n",
       "      <a href=\"/search/?searchtype=author&amp;query=Wu%2C+Y\">Yonghui Wu</a>\n",
       "</p>\n",
       "<p class=\"abstract mathjax\">\n",
       "<span class=\"search-hit\">Abstract</span>:\n",
       "      <span class=\"abstract-short has-text-grey-dark mathjax\" id=\"2002.03788v1-abstract-short\" style=\"display: inline;\">\n",
       "        …models with fine-grained latent features enable precise control of the prosody of synthesized speech. Such models typically incorporate a fine-grained variational autoencoder (<span class=\"search-hit mathjax\">VAE</span>) structure, extracting latent features at each input token (e.g., phonemes). However, generating samples with the standard…\n",
       "        <a class=\"is-size-7\" onclick=\"document.getElementById('2002.03788v1-abstract-full').style.display = 'inline'; document.getElementById('2002.03788v1-abstract-short').style.display = 'none';\" style=\"white-space: nowrap;\">▽ More</a>\n",
       "</span>\n",
       "<span class=\"abstract-full has-text-grey-dark mathjax\" id=\"2002.03788v1-abstract-full\" style=\"display: none;\">\n",
       "        Recent neural text-to-speech (TTS) models with fine-grained latent features enable precise control of the prosody of synthesized speech. Such models typically incorporate a fine-grained variational autoencoder (<span class=\"search-hit mathjax\">VAE</span>) structure, extracting latent features at each input token (e.g., phonemes). However, generating samples with the standard <span class=\"search-hit mathjax\">VAE</span> prior often results in unnatural and discontinuous speech, with dramatic prosodic variation between tokens. This paper proposes a sequential prior in a discrete latent space which can generate more naturally sounding samples. This is accomplished by discretizing the latent features using vector quantization (<span class=\"search-hit mathjax\">VQ</span>), and separately training an autoregressive (AR) prior model over the result. We evaluate the approach using listening tests, objective metrics of automatic speech recognition (ASR) performance, and measurements of prosody attributes. Experimental results show that the proposed model significantly improves the naturalness in random sample generation. Furthermore, initial experiments demonstrate that randomly sampling from the proposed model can be used as data augmentation to improve the ASR performance.\n",
       "        <a class=\"is-size-7\" onclick=\"document.getElementById('2002.03788v1-abstract-full').style.display = 'none'; document.getElementById('2002.03788v1-abstract-short').style.display = 'inline';\" style=\"white-space: nowrap;\">△ Less</a>\n",
       "</span>\n",
       "</p>\n",
       "<p class=\"is-size-7\"><span class=\"has-text-black-bis has-text-weight-semibold\">Submitted</span> 6 February, 2020; \n",
       "      <span class=\"has-text-black-bis has-text-weight-semibold\">originally announced</span> February 2020.\n",
       "      \n",
       "    </p>\n",
       "<p class=\"comments is-size-7\">\n",
       "<span class=\"has-text-black-bis has-text-weight-semibold\">Comments:</span>\n",
       "<span class=\"has-text-grey-dark mathjax\">To appear in ICASSP 2020</span>\n",
       "</p>\n",
       "</li>\n",
       "<li class=\"arxiv-result\">\n",
       "<div class=\"is-marginless\">\n",
       "<p class=\"list-title is-inline-block\"><a href=\"https://arxiv.org/abs/2001.08477\">arXiv:2001.08477</a>\n",
       "<span> [<a href=\"https://arxiv.org/pdf/2001.08477\">pdf</a>, <a href=\"https://arxiv.org/format/2001.08477\">other</a>] </span>\n",
       "</p>\n",
       "<div class=\"tags is-inline-block\">\n",
       "<span class=\"tag is-small is-link tooltip is-tooltip-top\" data-tooltip=\"Machine Learning\">cs.LG</span>\n",
       "<span class=\"tag is-small is-grey tooltip is-tooltip-top\" data-tooltip=\"Robotics\">cs.RO</span>\n",
       "<span class=\"tag is-small is-grey tooltip is-tooltip-top\" data-tooltip=\"Machine Learning\">stat.ML</span>\n",
       "</div>\n",
       "</div>\n",
       "<p class=\"title is-5 mathjax\">\n",
       "      \n",
       "        Semi-supervised Grasp Detection by Representation Learning in a Vector Quantized Latent Space\n",
       "      \n",
       "    </p>\n",
       "<p class=\"authors\">\n",
       "<span class=\"has-text-black-bis has-text-weight-semibold\">Authors:</span>\n",
       "<a href=\"/search/?searchtype=author&amp;query=Mahajan%2C+M\">Mridul Mahajan</a>, \n",
       "      \n",
       "      <a href=\"/search/?searchtype=author&amp;query=Bhattacharjee%2C+T\">Tryambak Bhattacharjee</a>, \n",
       "      \n",
       "      <a href=\"/search/?searchtype=author&amp;query=Krishnan%2C+A\">Arya Krishnan</a>, \n",
       "      \n",
       "      <a href=\"/search/?searchtype=author&amp;query=Shukla%2C+P\">Priya Shukla</a>, \n",
       "      \n",
       "      <a href=\"/search/?searchtype=author&amp;query=Nandi%2C+G+C\">G C Nandi</a>\n",
       "</p>\n",
       "<p class=\"abstract mathjax\">\n",
       "<span class=\"search-hit\">Abstract</span>:\n",
       "      <span class=\"abstract-short has-text-grey-dark mathjax\" id=\"2001.08477v3-abstract-short\" style=\"display: inline;\">\n",
       "        …this paper, a semi-supervised learning based grasp detection approach has been presented, which models a discrete latent space using a Vector Quantized Variational AutoEncoder (<span class=\"search-hit mathjax\">VQ</span>-…\n",
       "        <a class=\"is-size-7\" onclick=\"document.getElementById('2001.08477v3-abstract-full').style.display = 'inline'; document.getElementById('2001.08477v3-abstract-short').style.display = 'none';\" style=\"white-space: nowrap;\">▽ More</a>\n",
       "</span>\n",
       "<span class=\"abstract-full has-text-grey-dark mathjax\" id=\"2001.08477v3-abstract-full\" style=\"display: none;\">\n",
       "        For a robot to perform complex manipulation tasks, it is necessary for it to have a good grasping ability. However, vision based robotic grasp detection is hindered by the unavailability of sufficient labelled data. Furthermore, the application of semi-supervised learning techniques to grasp detection is under-explored. In this paper, a semi-supervised learning based grasp detection approach has been presented, which models a discrete latent space using a Vector Quantized Variational AutoEncoder (<span class=\"search-hit mathjax\">VQ</span>-<span class=\"search-hit mathjax\">VAE</span>). To the best of our knowledge, this is the first time a Variational AutoEncoder (<span class=\"search-hit mathjax\">VAE</span>) has been applied in the domain of robotic grasp detection. The <span class=\"search-hit mathjax\">VAE</span> helps the model in generalizing beyond the Cornell Grasping Dataset (CGD) despite having a limited amount of labelled data by also utilizing the unlabelled data. This claim has been validated by testing the model on images, which are not available in the CGD. Along with this, we augment the Generative Grasping Convolutional Neural Network (GGCNN) architecture with the decoder structure used in the <span class=\"search-hit mathjax\">VQ</span>-<span class=\"search-hit mathjax\">VAE</span> model with the intuition that it should help to regress in the vector-quantized latent space. Subsequently, the model performs significantly better than the existing approaches which do not make use of unlabelled images to improve the grasp.\n",
       "        <a class=\"is-size-7\" onclick=\"document.getElementById('2001.08477v3-abstract-full').style.display = 'none'; document.getElementById('2001.08477v3-abstract-short').style.display = 'inline';\" style=\"white-space: nowrap;\">△ Less</a>\n",
       "</span>\n",
       "</p>\n",
       "<p class=\"is-size-7\"><span class=\"has-text-black-bis has-text-weight-semibold\">Submitted</span> 30 January, 2020; <span class=\"has-text-black-bis has-text-weight-semibold\">v1</span> submitted 23 January, 2020;\n",
       "      <span class=\"has-text-black-bis has-text-weight-semibold\">originally announced</span> January 2020.\n",
       "      \n",
       "    </p>\n",
       "</li>\n",
       "<li class=\"arxiv-result\">\n",
       "<div class=\"is-marginless\">\n",
       "<p class=\"list-title is-inline-block\"><a href=\"https://arxiv.org/abs/1910.06464\">arXiv:1910.06464</a>\n",
       "<span> [<a href=\"https://arxiv.org/pdf/1910.06464\">pdf</a>, <a href=\"https://arxiv.org/format/1910.06464\">other</a>] </span>\n",
       "</p>\n",
       "<div class=\"tags is-inline-block\">\n",
       "<span class=\"tag is-small is-link tooltip is-tooltip-top\" data-tooltip=\"Machine Learning\">cs.LG</span>\n",
       "<span class=\"tag is-small is-grey tooltip is-tooltip-top\" data-tooltip=\"Sound\">cs.SD</span>\n",
       "<span class=\"tag is-small is-grey tooltip is-tooltip-top\" data-tooltip=\"Audio and Speech Processing\">eess.AS</span>\n",
       "<span class=\"tag is-small is-grey tooltip is-tooltip-top\" data-tooltip=\"Machine Learning\">stat.ML</span>\n",
       "</div>\n",
       "<div class=\"is-inline-block\" style=\"margin-left: 0.5rem\">\n",
       "<div class=\"tags has-addons\">\n",
       "<span class=\"tag is-dark is-size-7\">doi</span>\n",
       "<span class=\"tag is-light is-size-7\"><a class=\"\" href=\"https://doi.org/10.1109/ICASSP.2019.8683277\">10.1109/ICASSP.2019.8683277 <i aria-hidden=\"true\" class=\"fa fa-external-link\"></i></a></span>\n",
       "</div>\n",
       "</div>\n",
       "</div>\n",
       "<p class=\"title is-5 mathjax\">\n",
       "      \n",
       "        Low Bit-Rate Speech Coding with <span class=\"search-hit mathjax\">VQ</span>-<span class=\"search-hit mathjax\">VAE</span> and a WaveNet Decoder\n",
       "      \n",
       "    </p>\n",
       "<p class=\"authors\">\n",
       "<span class=\"has-text-black-bis has-text-weight-semibold\">Authors:</span>\n",
       "<a href=\"/search/?searchtype=author&amp;query=G%C3%A2rbacea%2C+C\">Cristina Gârbacea</a>, \n",
       "      \n",
       "      <a href=\"/search/?searchtype=author&amp;query=Oord%2C+A+v+d\">Aäron van den Oord</a>, \n",
       "      \n",
       "      <a href=\"/search/?searchtype=author&amp;query=Li%2C+Y\">Yazhe Li</a>, \n",
       "      \n",
       "      <a href=\"/search/?searchtype=author&amp;query=Lim%2C+F+S+C\">Felicia S C Lim</a>, \n",
       "      \n",
       "      <a href=\"/search/?searchtype=author&amp;query=Luebs%2C+A\">Alejandro Luebs</a>, \n",
       "      \n",
       "      <a href=\"/search/?searchtype=author&amp;query=Vinyals%2C+O\">Oriol Vinyals</a>, \n",
       "      \n",
       "      <a href=\"/search/?searchtype=author&amp;query=Walters%2C+T+C\">Thomas C Walters</a>\n",
       "</p>\n",
       "<p class=\"abstract mathjax\">\n",
       "<span class=\"search-hit\">Abstract</span>:\n",
       "      <span class=\"abstract-short has-text-grey-dark mathjax\" id=\"1910.06464v1-abstract-short\" style=\"display: inline;\">\n",
       "        …of the input signal which is then decoded at the receiver with the best possible perceptual quality. In this work we demonstrate that a neural network architecture based on <span class=\"search-hit mathjax\">VQ</span>-<span class=\"search-hit mathjax\">VAE</span> with a WaveNet decoder can be used to perform very low bit-rate speech coding with high reconstruction quality. A prosody-transparent and sp…\n",
       "        <a class=\"is-size-7\" onclick=\"document.getElementById('1910.06464v1-abstract-full').style.display = 'inline'; document.getElementById('1910.06464v1-abstract-short').style.display = 'none';\" style=\"white-space: nowrap;\">▽ More</a>\n",
       "</span>\n",
       "<span class=\"abstract-full has-text-grey-dark mathjax\" id=\"1910.06464v1-abstract-full\" style=\"display: none;\">\n",
       "        In order to efficiently transmit and store speech signals, speech codecs create a minimally redundant representation of the input signal which is then decoded at the receiver with the best possible perceptual quality. In this work we demonstrate that a neural network architecture based on <span class=\"search-hit mathjax\">VQ</span>-<span class=\"search-hit mathjax\">VAE</span> with a WaveNet decoder can be used to perform very low bit-rate speech coding with high reconstruction quality. A prosody-transparent and speaker-independent model trained on the LibriSpeech corpus coding audio at 1.6 kbps exhibits perceptual quality which is around halfway between the MELP codec at 2.4 kbps and AMR-WB codec at 23.05 kbps. In addition, when training on high-quality recorded speech with the test speaker included in the training set, a model coding speech at 1.6 kbps produces output of similar perceptual quality to that generated by AMR-WB at 23.05 kbps.\n",
       "        <a class=\"is-size-7\" onclick=\"document.getElementById('1910.06464v1-abstract-full').style.display = 'none'; document.getElementById('1910.06464v1-abstract-short').style.display = 'inline';\" style=\"white-space: nowrap;\">△ Less</a>\n",
       "</span>\n",
       "</p>\n",
       "<p class=\"is-size-7\"><span class=\"has-text-black-bis has-text-weight-semibold\">Submitted</span> 14 October, 2019; \n",
       "      <span class=\"has-text-black-bis has-text-weight-semibold\">originally announced</span> October 2019.\n",
       "      \n",
       "    </p>\n",
       "<p class=\"comments is-size-7\">\n",
       "<span class=\"has-text-black-bis has-text-weight-semibold\">Comments:</span>\n",
       "<span class=\"has-text-grey-dark mathjax\">ICASSP 2019</span>\n",
       "</p>\n",
       "<p class=\"comments is-size-7\">\n",
       "<span class=\"has-text-black-bis has-text-weight-semibold\">Journal ref:</span>\n",
       "        ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 735-739. IEEE, 2019\n",
       "      </p>\n",
       "</li>\n",
       "<li class=\"arxiv-result\">\n",
       "<div class=\"is-marginless\">\n",
       "<p class=\"list-title is-inline-block\"><a href=\"https://arxiv.org/abs/1909.11124\">arXiv:1909.11124</a>\n",
       "<span> [<a href=\"https://arxiv.org/pdf/1909.11124\">pdf</a>] </span>\n",
       "</p>\n",
       "<div class=\"tags is-inline-block\">\n",
       "<span class=\"tag is-small is-link tooltip is-tooltip-top\" data-tooltip=\"Machine Learning\">cs.LG</span>\n",
       "<span class=\"tag is-small is-grey tooltip is-tooltip-top\" data-tooltip=\"Machine Learning\">stat.ML</span>\n",
       "</div>\n",
       "</div>\n",
       "<p class=\"title is-5 mathjax\">\n",
       "      \n",
       "        Supervised Vector Quantized Variational Autoencoder for Learning Interpretable Global Representations\n",
       "      \n",
       "    </p>\n",
       "<p class=\"authors\">\n",
       "<span class=\"has-text-black-bis has-text-weight-semibold\">Authors:</span>\n",
       "<a href=\"/search/?searchtype=author&amp;query=Xue%2C+Y\">Yifan Xue</a>, \n",
       "      \n",
       "      <a href=\"/search/?searchtype=author&amp;query=Ding%2C+M\">Michael Ding</a>, \n",
       "      \n",
       "      <a href=\"/search/?searchtype=author&amp;query=Lu%2C+X\">Xinghua Lu</a>\n",
       "</p>\n",
       "<p class=\"abstract mathjax\">\n",
       "<span class=\"search-hit\">Abstract</span>:\n",
       "      <span class=\"abstract-short has-text-grey-dark mathjax\" id=\"1909.11124v2-abstract-short\" style=\"display: inline;\">\n",
       "        …each class of data is also a process of acquiring knowledge. Here, we present a novel generative model, referred to as the Supervised Vector Quantized Variational AutoEncoder (S-<span class=\"search-hit mathjax\">VQ</span>-…\n",
       "        <a class=\"is-size-7\" onclick=\"document.getElementById('1909.11124v2-abstract-full').style.display = 'inline'; document.getElementById('1909.11124v2-abstract-short').style.display = 'none';\" style=\"white-space: nowrap;\">▽ More</a>\n",
       "</span>\n",
       "<span class=\"abstract-full has-text-grey-dark mathjax\" id=\"1909.11124v2-abstract-full\" style=\"display: none;\">\n",
       "        Learning interpretable representations of data remains a central challenge in deep learning. When training a deep generative model, the observed data are often associated with certain categorical labels, and, in parallel with learning to regenerate data and simulate new data, learning an interpretable representation of each class of data is also a process of acquiring knowledge. Here, we present a novel generative model, referred to as the Supervised Vector Quantized Variational AutoEncoder (S-<span class=\"search-hit mathjax\">VQ</span>-<span class=\"search-hit mathjax\">VAE</span>), which combines the power of supervised and unsupervised learning to obtain a unique, interpretable global representation for each class of data. Compared with conventional generative models, our model has three key advantages: first, it is an integrative model that can simultaneously learn a feature representation for individual data point and a global representation for each class of data; second, the learning of global representations with embedding codes is guided by supervised information, which clearly defines the interpretation of each code; and third, the global representations capture crucial characteristics of different classes, which reveal similarity and differences of statistical structures underlying different groups of data. We evaluated the utility of S-<span class=\"search-hit mathjax\">VQ</span>-<span class=\"search-hit mathjax\">VAE</span> on a machine learning benchmark dataset, the MNIST dataset, and on gene expression data from the Library of Integrated Network-Based Cellular Signatures (LINCS). We proved that S-<span class=\"search-hit mathjax\">VQ</span>-<span class=\"search-hit mathjax\">VAE</span> was able to learn the global genetic characteristics of samples perturbed by the same class of perturbagen (PCL), and further revealed the mechanism correlations between PCLs. Such knowledge is crucial for promoting new drug development for complex diseases like cancer.\n",
       "        <a class=\"is-size-7\" onclick=\"document.getElementById('1909.11124v2-abstract-full').style.display = 'none'; document.getElementById('1909.11124v2-abstract-short').style.display = 'inline';\" style=\"white-space: nowrap;\">△ Less</a>\n",
       "</span>\n",
       "</p>\n",
       "<p class=\"is-size-7\"><span class=\"has-text-black-bis has-text-weight-semibold\">Submitted</span> 29 September, 2019; <span class=\"has-text-black-bis has-text-weight-semibold\">v1</span> submitted 24 September, 2019;\n",
       "      <span class=\"has-text-black-bis has-text-weight-semibold\">originally announced</span> September 2019.\n",
       "      \n",
       "    </p>\n",
       "</li>\n",
       "<li class=\"arxiv-result\">\n",
       "<div class=\"is-marginless\">\n",
       "<p class=\"list-title is-inline-block\"><a href=\"https://arxiv.org/abs/1909.03522\">arXiv:1909.03522</a>\n",
       "<span> [<a href=\"https://arxiv.org/pdf/1909.03522\">pdf</a>, <a href=\"https://arxiv.org/format/1909.03522\">other</a>] </span>\n",
       "</p>\n",
       "<div class=\"tags is-inline-block\">\n",
       "<span class=\"tag is-small is-link tooltip is-tooltip-top\" data-tooltip=\"Machine Learning\">cs.LG</span>\n",
       "<span class=\"tag is-small is-grey tooltip is-tooltip-top\" data-tooltip=\"Sound\">cs.SD</span>\n",
       "<span class=\"tag is-small is-grey tooltip is-tooltip-top\" data-tooltip=\"Audio and Speech Processing\">eess.AS</span>\n",
       "</div>\n",
       "</div>\n",
       "<p class=\"title is-5 mathjax\">\n",
       "      \n",
       "        MIDI-Sandwich2: RNN-based Hierarchical Multi-modal Fusion Generation <span class=\"search-hit mathjax\">VAE</span> networks for multi-track symbolic music generation\n",
       "      \n",
       "    </p>\n",
       "<p class=\"authors\">\n",
       "<span class=\"has-text-black-bis has-text-weight-semibold\">Authors:</span>\n",
       "<a href=\"/search/?searchtype=author&amp;query=Liang%2C+X\">Xia Liang</a>, \n",
       "      \n",
       "      <a href=\"/search/?searchtype=author&amp;query=Wu%2C+J\">Junmin Wu</a>, \n",
       "      \n",
       "      <a href=\"/search/?searchtype=author&amp;query=Cao%2C+J\">Jing Cao</a>\n",
       "</p>\n",
       "<p class=\"abstract mathjax\">\n",
       "<span class=\"search-hit\">Abstract</span>:\n",
       "      <span class=\"abstract-short has-text-grey-dark mathjax\" id=\"1909.03522v1-abstract-short\" style=\"display: inline;\">\n",
       "        …based models can not be applied in this task. In view of the above problem, this paper proposes a RNN-based Hierarchical Multi-modal Fusion Generation Variational Autoencoder (<span class=\"search-hit mathjax\">VAE</span>) network, MIDI-Sandwich2, for multi-track symbolic music generation. Inspired by…\n",
       "        <a class=\"is-size-7\" onclick=\"document.getElementById('1909.03522v1-abstract-full').style.display = 'inline'; document.getElementById('1909.03522v1-abstract-short').style.display = 'none';\" style=\"white-space: nowrap;\">▽ More</a>\n",
       "</span>\n",
       "<span class=\"abstract-full has-text-grey-dark mathjax\" id=\"1909.03522v1-abstract-full\" style=\"display: none;\">\n",
       "        Currently, almost all the multi-track music generation models use the Convolutional Neural Network (CNN) to build the generative model, while the Recurrent Neural Network (RNN) based models can not be applied in this task. In view of the above problem, this paper proposes a RNN-based Hierarchical Multi-modal Fusion Generation Variational Autoencoder (<span class=\"search-hit mathjax\">VAE</span>) network, MIDI-Sandwich2, for multi-track symbolic music generation. Inspired by <span class=\"search-hit mathjax\">VQ</span>-VAE2, MIDI-Sandwich2 expands the dimension of the original hierarchical model by using multiple independent Binary Variational Autoencoder (BVAE) models without sharing weights to process the information of each track. Then, with multi-modal fusion technology, the upper layer named Multi-modal Fusion Generation <span class=\"search-hit mathjax\">VAE</span> (MFG-<span class=\"search-hit mathjax\">VAE</span>) combines the latent space vectors generated by the respective tracks, and uses the decoder to perform the ascending dimension reconstruction to simulate the inverse operation of multi-modal fusion, multi-modal generation, so as to realize the RNN-based multi-track symbolic music generation. For the multi-track format pianoroll, we also improve the output binarization method of MuseGAN, which solves the problem that the refinement step of the original scheme is difficult to differentiate and the gradient is hard to descent, making the generated song more expressive. The model is validated on the Lakh Pianoroll Dataset (LPD) multi-track dataset. Compared to the MuseGAN, MIDI-Sandwich2 can not only generate harmonious multi-track music, the generation quality is also close to the state of the art level. At the same time, by using the <span class=\"search-hit mathjax\">VAE</span> to restore songs, the semi-generated songs reproduced by the MIDI-Sandwich2 are more beautiful than the pure autogeneration music generated by MuseGAN. Both the code and the audition audio samples are open source on https://github.com/LiangHsia/MIDI-S2.\n",
       "        <a class=\"is-size-7\" onclick=\"document.getElementById('1909.03522v1-abstract-full').style.display = 'none'; document.getElementById('1909.03522v1-abstract-short').style.display = 'inline';\" style=\"white-space: nowrap;\">△ Less</a>\n",
       "</span>\n",
       "</p>\n",
       "<p class=\"is-size-7\"><span class=\"has-text-black-bis has-text-weight-semibold\">Submitted</span> 8 September, 2019; \n",
       "      <span class=\"has-text-black-bis has-text-weight-semibold\">originally announced</span> September 2019.\n",
       "      \n",
       "    </p>\n",
       "</li>\n",
       "<li class=\"arxiv-result\">\n",
       "<div class=\"is-marginless\">\n",
       "<p class=\"list-title is-inline-block\"><a href=\"https://arxiv.org/abs/1906.00446\">arXiv:1906.00446</a>\n",
       "<span> [<a href=\"https://arxiv.org/pdf/1906.00446\">pdf</a>, <a href=\"https://arxiv.org/format/1906.00446\">other</a>] </span>\n",
       "</p>\n",
       "<div class=\"tags is-inline-block\">\n",
       "<span class=\"tag is-small is-link tooltip is-tooltip-top\" data-tooltip=\"Machine Learning\">cs.LG</span>\n",
       "<span class=\"tag is-small is-grey tooltip is-tooltip-top\" data-tooltip=\"Computer Vision and Pattern Recognition\">cs.CV</span>\n",
       "<span class=\"tag is-small is-grey tooltip is-tooltip-top\" data-tooltip=\"Machine Learning\">stat.ML</span>\n",
       "</div>\n",
       "</div>\n",
       "<p class=\"title is-5 mathjax\">\n",
       "      \n",
       "        Generating Diverse High-Fidelity Images with <span class=\"search-hit mathjax\">VQ</span>-<span class=\"search-hit mathjax\">VAE</span>-2\n",
       "      \n",
       "    </p>\n",
       "<p class=\"authors\">\n",
       "<span class=\"has-text-black-bis has-text-weight-semibold\">Authors:</span>\n",
       "<a href=\"/search/?searchtype=author&amp;query=Razavi%2C+A\">Ali Razavi</a>, \n",
       "      \n",
       "      <a href=\"/search/?searchtype=author&amp;query=Oord%2C+A+v+d\">Aaron van den Oord</a>, \n",
       "      \n",
       "      <a href=\"/search/?searchtype=author&amp;query=Vinyals%2C+O\">Oriol Vinyals</a>\n",
       "</p>\n",
       "<p class=\"abstract mathjax\">\n",
       "<span class=\"search-hit\">Abstract</span>:\n",
       "      <span class=\"abstract-short has-text-grey-dark mathjax\" id=\"1906.00446v1-abstract-short\" style=\"display: inline;\">\n",
       "        We explore the use of Vector Quantized Variational AutoEncoder (<span class=\"search-hit mathjax\">VQ</span>-…\n",
       "        <a class=\"is-size-7\" onclick=\"document.getElementById('1906.00446v1-abstract-full').style.display = 'inline'; document.getElementById('1906.00446v1-abstract-short').style.display = 'none';\" style=\"white-space: nowrap;\">▽ More</a>\n",
       "</span>\n",
       "<span class=\"abstract-full has-text-grey-dark mathjax\" id=\"1906.00446v1-abstract-full\" style=\"display: none;\">\n",
       "        We explore the use of Vector Quantized Variational AutoEncoder (<span class=\"search-hit mathjax\">VQ</span>-<span class=\"search-hit mathjax\">VAE</span>) models for large scale image generation. To this end, we scale and enhance the autoregressive priors used in <span class=\"search-hit mathjax\">VQ</span>-<span class=\"search-hit mathjax\">VAE</span> to generate synthetic samples of much higher coherence and fidelity than possible before. We use simple feed-forward encoder and decoder networks, making our model an attractive candidate for applications where the encoding and/or decoding speed is critical. Additionally, <span class=\"search-hit mathjax\">VQ</span>-<span class=\"search-hit mathjax\">VAE</span> requires sampling an autoregressive model only in the compressed latent space, which is an order of magnitude faster than sampling in the pixel space, especially for large images. We demonstrate that a multi-scale hierarchical organization of <span class=\"search-hit mathjax\">VQ</span>-<span class=\"search-hit mathjax\">VAE</span>, augmented with powerful priors over the latent codes, is able to generate samples with quality that rivals that of state of the art Generative Adversarial Networks on multifaceted datasets such as ImageNet, while not suffering from GAN's known shortcomings such as mode collapse and lack of diversity.\n",
       "        <a class=\"is-size-7\" onclick=\"document.getElementById('1906.00446v1-abstract-full').style.display = 'none'; document.getElementById('1906.00446v1-abstract-short').style.display = 'inline';\" style=\"white-space: nowrap;\">△ Less</a>\n",
       "</span>\n",
       "</p>\n",
       "<p class=\"is-size-7\"><span class=\"has-text-black-bis has-text-weight-semibold\">Submitted</span> 2 June, 2019; \n",
       "      <span class=\"has-text-black-bis has-text-weight-semibold\">originally announced</span> June 2019.\n",
       "      \n",
       "    </p>\n",
       "</li>\n",
       "<li class=\"arxiv-result\">\n",
       "<div class=\"is-marginless\">\n",
       "<p class=\"list-title is-inline-block\"><a href=\"https://arxiv.org/abs/1905.11449\">arXiv:1905.11449</a>\n",
       "<span> [<a href=\"https://arxiv.org/pdf/1905.11449\">pdf</a>, <a href=\"https://arxiv.org/format/1905.11449\">other</a>] </span>\n",
       "</p>\n",
       "<div class=\"tags is-inline-block\">\n",
       "<span class=\"tag is-small is-link tooltip is-tooltip-top\" data-tooltip=\"Computation and Language\">cs.CL</span>\n",
       "<span class=\"tag is-small is-grey tooltip is-tooltip-top\" data-tooltip=\"Machine Learning\">cs.LG</span>\n",
       "<span class=\"tag is-small is-grey tooltip is-tooltip-top\" data-tooltip=\"Sound\">cs.SD</span>\n",
       "<span class=\"tag is-small is-grey tooltip is-tooltip-top\" data-tooltip=\"Audio and Speech Processing\">eess.AS</span>\n",
       "</div>\n",
       "</div>\n",
       "<p class=\"title is-5 mathjax\">\n",
       "      \n",
       "        VQVAE Unsupervised Unit Discovery and Multi-scale Code2Spec Inverter for Zerospeech Challenge 2019\n",
       "      \n",
       "    </p>\n",
       "<p class=\"authors\">\n",
       "<span class=\"has-text-black-bis has-text-weight-semibold\">Authors:</span>\n",
       "<a href=\"/search/?searchtype=author&amp;query=Tjandra%2C+A\">Andros Tjandra</a>, \n",
       "      \n",
       "      <a href=\"/search/?searchtype=author&amp;query=Sisman%2C+B\">Berrak Sisman</a>, \n",
       "      \n",
       "      <a href=\"/search/?searchtype=author&amp;query=Zhang%2C+M\">Mingyang Zhang</a>, \n",
       "      \n",
       "      <a href=\"/search/?searchtype=author&amp;query=Sakti%2C+S\">Sakriani Sakti</a>, \n",
       "      \n",
       "      <a href=\"/search/?searchtype=author&amp;query=Li%2C+H\">Haizhou Li</a>, \n",
       "      \n",
       "      <a href=\"/search/?searchtype=author&amp;query=Nakamura%2C+S\">Satoshi Nakamura</a>\n",
       "</p>\n",
       "<p class=\"abstract mathjax\">\n",
       "<span class=\"search-hit\">Abstract</span>:\n",
       "      <span class=\"abstract-short has-text-grey-dark mathjax\" id=\"1905.11449v2-abstract-short\" style=\"display: inline;\">\n",
       "        …the naturalness and the intelligibility of the constructed voice. To tackle these problems and achieve the best trade-off, we utilize a vector quantized variational autoencoder (<span class=\"search-hit mathjax\">VQ</span>-…\n",
       "        <a class=\"is-size-7\" onclick=\"document.getElementById('1905.11449v2-abstract-full').style.display = 'inline'; document.getElementById('1905.11449v2-abstract-short').style.display = 'none';\" style=\"white-space: nowrap;\">▽ More</a>\n",
       "</span>\n",
       "<span class=\"abstract-full has-text-grey-dark mathjax\" id=\"1905.11449v2-abstract-full\" style=\"display: none;\">\n",
       "        We describe our submitted system for the ZeroSpeech Challenge 2019. The current challenge theme addresses the difficulty of constructing a speech synthesizer without any text or phonetic labels and requires a system that can (1) discover subword units in an unsupervised way, and (2) synthesize the speech with a target speaker's voice. Moreover, the system should also balance the discrimination score ABX, the bit-rate compression rate, and the naturalness and the intelligibility of the constructed voice. To tackle these problems and achieve the best trade-off, we utilize a vector quantized variational autoencoder (<span class=\"search-hit mathjax\">VQ</span>-<span class=\"search-hit mathjax\">VAE</span>) and a multi-scale codebook-to-spectrogram (Code2Spec) inverter trained by mean square error and adversarial loss. The <span class=\"search-hit mathjax\">VQ</span>-<span class=\"search-hit mathjax\">VAE</span> extracts the speech to a latent space, forces itself to map it into the nearest codebook and produces compressed representation. Next, the inverter generates a magnitude spectrogram to the target voice, given the codebook vectors from <span class=\"search-hit mathjax\">VQ</span>-<span class=\"search-hit mathjax\">VAE</span>. In our experiments, we also investigated several other clustering algorithms, including K-Means and GMM, and compared them with the <span class=\"search-hit mathjax\">VQ</span>-<span class=\"search-hit mathjax\">VAE</span> result on ABX scores and bit rates. Our proposed approach significantly improved the intelligibility (in CER), the MOS, and discrimination ABX scores compared to the official ZeroSpeech 2019 baseline or even the topline.\n",
       "        <a class=\"is-size-7\" onclick=\"document.getElementById('1905.11449v2-abstract-full').style.display = 'none'; document.getElementById('1905.11449v2-abstract-short').style.display = 'inline';\" style=\"white-space: nowrap;\">△ Less</a>\n",
       "</span>\n",
       "</p>\n",
       "<p class=\"is-size-7\"><span class=\"has-text-black-bis has-text-weight-semibold\">Submitted</span> 29 May, 2019; <span class=\"has-text-black-bis has-text-weight-semibold\">v1</span> submitted 27 May, 2019;\n",
       "      <span class=\"has-text-black-bis has-text-weight-semibold\">originally announced</span> May 2019.\n",
       "      \n",
       "    </p>\n",
       "<p class=\"comments is-size-7\">\n",
       "<span class=\"has-text-black-bis has-text-weight-semibold\">Comments:</span>\n",
       "<span class=\"has-text-grey-dark mathjax\">Submitted to Interspeech 2019</span>\n",
       "</p>\n",
       "</li>\n",
       "<li class=\"arxiv-result\">\n",
       "<div class=\"is-marginless\">\n",
       "<p class=\"list-title is-inline-block\"><a href=\"https://arxiv.org/abs/1905.11062\">arXiv:1905.11062</a>\n",
       "<span> [<a href=\"https://arxiv.org/pdf/1905.11062\">pdf</a>, <a href=\"https://arxiv.org/format/1905.11062\">other</a>] </span>\n",
       "</p>\n",
       "<div class=\"tags is-inline-block\">\n",
       "<span class=\"tag is-small is-link tooltip is-tooltip-top\" data-tooltip=\"Machine Learning\">cs.LG</span>\n",
       "<span class=\"tag is-small is-grey tooltip is-tooltip-top\" data-tooltip=\"Machine Learning\">stat.ML</span>\n",
       "</div>\n",
       "</div>\n",
       "<p class=\"title is-5 mathjax\">\n",
       "      \n",
       "        Quantization-Based Regularization for Autoencoders\n",
       "      \n",
       "    </p>\n",
       "<p class=\"authors\">\n",
       "<span class=\"has-text-black-bis has-text-weight-semibold\">Authors:</span>\n",
       "<a href=\"/search/?searchtype=author&amp;query=Wu%2C+H\">Hanwei Wu</a>, \n",
       "      \n",
       "      <a href=\"/search/?searchtype=author&amp;query=Flierl%2C+M\">Markus Flierl</a>\n",
       "</p>\n",
       "<p class=\"abstract mathjax\">\n",
       "<span class=\"search-hit\">Abstract</span>:\n",
       "      <span class=\"abstract-short has-text-grey-dark mathjax\" id=\"1905.11062v2-abstract-short\" style=\"display: inline;\">\n",
       "        …regularizer in the bottleneck stage of autoencoder models to learn meaningful latent representations. We combine both perspectives of Vector Quantized-Variational AutoEncoders (<span class=\"search-hit mathjax\">VQ</span>-<span class=\"search-hit mathjax\">VAE</span>) and classical denoising regularization methods of neural networks. We interpret quantizers as regularizers that constrain latent repres…\n",
       "        <a class=\"is-size-7\" onclick=\"document.getElementById('1905.11062v2-abstract-full').style.display = 'inline'; document.getElementById('1905.11062v2-abstract-short').style.display = 'none';\" style=\"white-space: nowrap;\">▽ More</a>\n",
       "</span>\n",
       "<span class=\"abstract-full has-text-grey-dark mathjax\" id=\"1905.11062v2-abstract-full\" style=\"display: none;\">\n",
       "        Autoencoders and their variations provide unsupervised models for learning low-dimensional representations for downstream tasks. Without proper regularization, autoencoder models are susceptible to the overfitting problem and the so-called posterior collapse phenomenon. In this paper, we introduce a quantization-based regularizer in the bottleneck stage of autoencoder models to learn meaningful latent representations. We combine both perspectives of Vector Quantized-Variational AutoEncoders (<span class=\"search-hit mathjax\">VQ</span>-<span class=\"search-hit mathjax\">VAE</span>) and classical denoising regularization methods of neural networks. We interpret quantizers as regularizers that constrain latent representations while fostering a similarity-preserving mapping at the encoder. Before quantization, we impose noise on the latent codes and use a Bayesian estimator to optimize the quantizer-based representation. The introduced bottleneck Bayesian estimator outputs the posterior mean of the centroids to the decoder, and thus, is performing soft quantization of the noisy latent codes. We show that our proposed regularization method results in improved latent representations for both supervised learning and clustering downstream tasks when compared to autoencoders using other bottleneck structures.\n",
       "        <a class=\"is-size-7\" onclick=\"document.getElementById('1905.11062v2-abstract-full').style.display = 'none'; document.getElementById('1905.11062v2-abstract-short').style.display = 'inline';\" style=\"white-space: nowrap;\">△ Less</a>\n",
       "</span>\n",
       "</p>\n",
       "<p class=\"is-size-7\"><span class=\"has-text-black-bis has-text-weight-semibold\">Submitted</span> 22 January, 2020; <span class=\"has-text-black-bis has-text-weight-semibold\">v1</span> submitted 27 May, 2019;\n",
       "      <span class=\"has-text-black-bis has-text-weight-semibold\">originally announced</span> May 2019.\n",
       "      \n",
       "    </p>\n",
       "<p class=\"comments is-size-7\">\n",
       "<span class=\"has-text-black-bis has-text-weight-semibold\">Comments:</span>\n",
       "<span class=\"has-text-grey-dark mathjax\">AAAI 2020</span>\n",
       "</p>\n",
       "</li>\n",
       "<li class=\"arxiv-result\">\n",
       "<div class=\"is-marginless\">\n",
       "<p class=\"list-title is-inline-block\"><a href=\"https://arxiv.org/abs/1905.10887\">arXiv:1905.10887</a>\n",
       "<span> [<a href=\"https://arxiv.org/pdf/1905.10887\">pdf</a>, <a href=\"https://arxiv.org/format/1905.10887\">other</a>] </span>\n",
       "</p>\n",
       "<div class=\"tags is-inline-block\">\n",
       "<span class=\"tag is-small is-link tooltip is-tooltip-top\" data-tooltip=\"Machine Learning\">cs.LG</span>\n",
       "<span class=\"tag is-small is-grey tooltip is-tooltip-top\" data-tooltip=\"Machine Learning\">stat.ML</span>\n",
       "</div>\n",
       "</div>\n",
       "<p class=\"title is-5 mathjax\">\n",
       "      \n",
       "        Classification Accuracy Score for Conditional Generative Models\n",
       "      \n",
       "    </p>\n",
       "<p class=\"authors\">\n",
       "<span class=\"has-text-black-bis has-text-weight-semibold\">Authors:</span>\n",
       "<a href=\"/search/?searchtype=author&amp;query=Ravuri%2C+S\">Suman Ravuri</a>, \n",
       "      \n",
       "      <a href=\"/search/?searchtype=author&amp;query=Vinyals%2C+O\">Oriol Vinyals</a>\n",
       "</p>\n",
       "<p class=\"abstract mathjax\">\n",
       "<span class=\"search-hit\">Abstract</span>:\n",
       "      <span class=\"abstract-short has-text-grey-dark mathjax\" id=\"1905.10887v2-abstract-short\" style=\"display: inline;\">\n",
       "        …by 27.9\\% and 41.6\\%, respectively, compared to the original data; and conditional generative models from other model classes, such as Vector-Quantized Variational Autoencoder-2 (<span class=\"search-hit mathjax\">VQ</span>-<span class=\"search-hit mathjax\">VAE</span>-2) and Hierarchical Autoregressive Models (HAMs), substantially outperform GANs on this benchmark. Second, CAS automatically surfaces…\n",
       "        <a class=\"is-size-7\" onclick=\"document.getElementById('1905.10887v2-abstract-full').style.display = 'inline'; document.getElementById('1905.10887v2-abstract-short').style.display = 'none';\" style=\"white-space: nowrap;\">▽ More</a>\n",
       "</span>\n",
       "<span class=\"abstract-full has-text-grey-dark mathjax\" id=\"1905.10887v2-abstract-full\" style=\"display: none;\">\n",
       "        Deep generative models (DGMs) of images are now sufficiently mature that they produce nearly photorealistic samples and obtain scores similar to the data distribution on heuristics such as Frechet Inception Distance (FID). These results, especially on large-scale datasets such as ImageNet, suggest that DGMs are learning the data distribution in a perceptually meaningful space and can be used in downstream tasks. To test this latter hypothesis, we use class-conditional generative models from a number of model classes---variational autoencoders, autoregressive models, and generative adversarial networks (GANs)---to infer the class labels of real data. We perform this inference by training an image classifier using only synthetic data and using the classifier to predict labels on real data. The performance on this task, which we call Classification Accuracy Score (CAS), reveals some surprising results not identified by traditional metrics and constitute our contributions. First, when using a state-of-the-art GAN (BigGAN-deep), Top-1 and Top-5 accuracy decrease by 27.9\\% and 41.6\\%, respectively, compared to the original data; and conditional generative models from other model classes, such as Vector-Quantized Variational Autoencoder-2 (<span class=\"search-hit mathjax\">VQ</span>-<span class=\"search-hit mathjax\">VAE</span>-2) and Hierarchical Autoregressive Models (HAMs), substantially outperform GANs on this benchmark. Second, CAS automatically surfaces particular classes for which generative models failed to capture the data distribution, and were previously unknown in the literature. Third, we find traditional GAN metrics such as Inception Score (IS) and FID neither predictive of CAS nor useful when evaluating non-GAN models. Furthermore, in order to facilitate better diagnoses of generative models, we open-source the proposed metric.\n",
       "        <a class=\"is-size-7\" onclick=\"document.getElementById('1905.10887v2-abstract-full').style.display = 'none'; document.getElementById('1905.10887v2-abstract-short').style.display = 'inline';\" style=\"white-space: nowrap;\">△ Less</a>\n",
       "</span>\n",
       "</p>\n",
       "<p class=\"is-size-7\"><span class=\"has-text-black-bis has-text-weight-semibold\">Submitted</span> 28 October, 2019; <span class=\"has-text-black-bis has-text-weight-semibold\">v1</span> submitted 26 May, 2019;\n",
       "      <span class=\"has-text-black-bis has-text-weight-semibold\">originally announced</span> May 2019.\n",
       "      \n",
       "    </p>\n",
       "</li>\n",
       "<li class=\"arxiv-result\">\n",
       "<div class=\"is-marginless\">\n",
       "<p class=\"list-title is-inline-block\"><a href=\"https://arxiv.org/abs/1904.07556\">arXiv:1904.07556</a>\n",
       "<span> [<a href=\"https://arxiv.org/pdf/1904.07556\">pdf</a>, <a href=\"https://arxiv.org/format/1904.07556\">other</a>] </span>\n",
       "</p>\n",
       "<div class=\"tags is-inline-block\">\n",
       "<span class=\"tag is-small is-link tooltip is-tooltip-top\" data-tooltip=\"Computation and Language\">cs.CL</span>\n",
       "<span class=\"tag is-small is-grey tooltip is-tooltip-top\" data-tooltip=\"Machine Learning\">cs.LG</span>\n",
       "<span class=\"tag is-small is-grey tooltip is-tooltip-top\" data-tooltip=\"Sound\">cs.SD</span>\n",
       "<span class=\"tag is-small is-grey tooltip is-tooltip-top\" data-tooltip=\"Audio and Speech Processing\">eess.AS</span>\n",
       "</div>\n",
       "</div>\n",
       "<p class=\"title is-5 mathjax\">\n",
       "      \n",
       "        Unsupervised acoustic unit discovery for speech synthesis using discrete latent-variable neural networks\n",
       "      \n",
       "    </p>\n",
       "<p class=\"authors\">\n",
       "<span class=\"has-text-black-bis has-text-weight-semibold\">Authors:</span>\n",
       "<a href=\"/search/?searchtype=author&amp;query=Eloff%2C+R\">Ryan Eloff</a>, \n",
       "      \n",
       "      <a href=\"/search/?searchtype=author&amp;query=Nortje%2C+A\">André Nortje</a>, \n",
       "      \n",
       "      <a href=\"/search/?searchtype=author&amp;query=van+Niekerk%2C+B\">Benjamin van Niekerk</a>, \n",
       "      \n",
       "      <a href=\"/search/?searchtype=author&amp;query=Govender%2C+A\">Avashna Govender</a>, \n",
       "      \n",
       "      <a href=\"/search/?searchtype=author&amp;query=Nortje%2C+L\">Leanne Nortje</a>, \n",
       "      \n",
       "      <a href=\"/search/?searchtype=author&amp;query=Pretorius%2C+A\">Arnu Pretorius</a>, \n",
       "      \n",
       "      <a href=\"/search/?searchtype=author&amp;query=van+Biljon%2C+E\">Elan van Biljon</a>, \n",
       "      \n",
       "      <a href=\"/search/?searchtype=author&amp;query=van+der+Westhuizen%2C+E\">Ewald van der Westhuizen</a>, \n",
       "      \n",
       "      <a href=\"/search/?searchtype=author&amp;query=van+Staden%2C+L\">Lisa van Staden</a>, \n",
       "      \n",
       "      <a href=\"/search/?searchtype=author&amp;query=Kamper%2C+H\">Herman Kamper</a>\n",
       "</p>\n",
       "<p class=\"abstract mathjax\">\n",
       "<span class=\"search-hit\">Abstract</span>:\n",
       "      <span class=\"abstract-short has-text-grey-dark mathjax\" id=\"1904.07556v2-abstract-short\" style=\"display: inline;\">\n",
       "        …output is fed to a neural vocoder to synthesise speech in the target speaker's voice. For discretisation, categorical variational autoencoders (CatVAEs), vector-quantised <span class=\"search-hit mathjax\">VAEs</span> (…\n",
       "        <a class=\"is-size-7\" onclick=\"document.getElementById('1904.07556v2-abstract-full').style.display = 'inline'; document.getElementById('1904.07556v2-abstract-short').style.display = 'none';\" style=\"white-space: nowrap;\">▽ More</a>\n",
       "</span>\n",
       "<span class=\"abstract-full has-text-grey-dark mathjax\" id=\"1904.07556v2-abstract-full\" style=\"display: none;\">\n",
       "        For our submission to the ZeroSpeech 2019 challenge, we apply discrete latent-variable neural networks to unlabelled speech and use the discovered units for speech synthesis. Unsupervised discrete subword modelling could be useful for studies of phonetic category learning in infants or in low-resource speech technology requiring symbolic input. We use an autoencoder (AE) architecture with intermediate discretisation. We decouple acoustic unit discovery from speaker modelling by conditioning the AE's decoder on the training speaker identity. At test time, unit discovery is performed on speech from an unseen speaker, followed by unit decoding conditioned on a known target speaker to obtain reconstructed filterbanks. This output is fed to a neural vocoder to synthesise speech in the target speaker's voice. For discretisation, categorical variational autoencoders (CatVAEs), vector-quantised <span class=\"search-hit mathjax\">VAEs</span> (<span class=\"search-hit mathjax\">VQ</span>-<span class=\"search-hit mathjax\">VAEs</span>) and straight-through estimation are compared at different compression levels on two languages. Our final model uses convolutional encoding, <span class=\"search-hit mathjax\">VQ</span>-<span class=\"search-hit mathjax\">VAE</span> discretisation, deconvolutional decoding and an FFTNet vocoder. We show that decoupled speaker conditioning intrinsically improves discrete acoustic representations, yielding competitive synthesis quality compared to the challenge baseline.\n",
       "        <a class=\"is-size-7\" onclick=\"document.getElementById('1904.07556v2-abstract-full').style.display = 'none'; document.getElementById('1904.07556v2-abstract-short').style.display = 'inline';\" style=\"white-space: nowrap;\">△ Less</a>\n",
       "</span>\n",
       "</p>\n",
       "<p class=\"is-size-7\"><span class=\"has-text-black-bis has-text-weight-semibold\">Submitted</span> 28 June, 2019; <span class=\"has-text-black-bis has-text-weight-semibold\">v1</span> submitted 16 April, 2019;\n",
       "      <span class=\"has-text-black-bis has-text-weight-semibold\">originally announced</span> April 2019.\n",
       "      \n",
       "    </p>\n",
       "<p class=\"comments is-size-7\">\n",
       "<span class=\"has-text-black-bis has-text-weight-semibold\">Comments:</span>\n",
       "<span class=\"has-text-grey-dark mathjax\">Interspeech 2019</span>\n",
       "</p>\n",
       "</li>\n",
       "<li class=\"arxiv-result\">\n",
       "<div class=\"is-marginless\">\n",
       "<p class=\"list-title is-inline-block\"><a href=\"https://arxiv.org/abs/1901.08810\">arXiv:1901.08810</a>\n",
       "<span> [<a href=\"https://arxiv.org/pdf/1901.08810\">pdf</a>, <a href=\"https://arxiv.org/format/1901.08810\">other</a>] </span>\n",
       "</p>\n",
       "<div class=\"tags is-inline-block\">\n",
       "<span class=\"tag is-small is-link tooltip is-tooltip-top\" data-tooltip=\"Machine Learning\">cs.LG</span>\n",
       "<span class=\"tag is-small is-grey tooltip is-tooltip-top\" data-tooltip=\"Audio and Speech Processing\">eess.AS</span>\n",
       "<span class=\"tag is-small is-grey tooltip is-tooltip-top\" data-tooltip=\"Machine Learning\">stat.ML</span>\n",
       "</div>\n",
       "<div class=\"is-inline-block\" style=\"margin-left: 0.5rem\">\n",
       "<div class=\"tags has-addons\">\n",
       "<span class=\"tag is-dark is-size-7\">doi</span>\n",
       "<span class=\"tag is-light is-size-7\"><a class=\"\" href=\"https://doi.org/10.1109/TASLP.2019.2938863\">10.1109/TASLP.2019.2938863 <i aria-hidden=\"true\" class=\"fa fa-external-link\"></i></a></span>\n",
       "</div>\n",
       "</div>\n",
       "</div>\n",
       "<p class=\"title is-5 mathjax\">\n",
       "      \n",
       "        Unsupervised speech representation learning using WaveNet autoencoders\n",
       "      \n",
       "    </p>\n",
       "<p class=\"authors\">\n",
       "<span class=\"has-text-black-bis has-text-weight-semibold\">Authors:</span>\n",
       "<a href=\"/search/?searchtype=author&amp;query=Chorowski%2C+J\">Jan Chorowski</a>, \n",
       "      \n",
       "      <a href=\"/search/?searchtype=author&amp;query=Weiss%2C+R+J\">Ron J. Weiss</a>, \n",
       "      \n",
       "      <a href=\"/search/?searchtype=author&amp;query=Bengio%2C+S\">Samy Bengio</a>, \n",
       "      \n",
       "      <a href=\"/search/?searchtype=author&amp;query=Oord%2C+A+v+d\">Aäron van den Oord</a>\n",
       "</p>\n",
       "<p class=\"abstract mathjax\">\n",
       "<span class=\"search-hit\">Abstract</span>:\n",
       "      <span class=\"abstract-short has-text-grey-dark mathjax\" id=\"1901.08810v2-abstract-short\" style=\"display: inline;\">\n",
       "        …the kind of constraint that is applied to the latent representation. We compare three variants: a simple dimensionality reduction bottleneck, a Gaussian Variational Autoencoder (<span class=\"search-hit mathjax\">VAE</span>), and a discrete Vector Quantized…\n",
       "        <a class=\"is-size-7\" onclick=\"document.getElementById('1901.08810v2-abstract-full').style.display = 'inline'; document.getElementById('1901.08810v2-abstract-short').style.display = 'none';\" style=\"white-space: nowrap;\">▽ More</a>\n",
       "</span>\n",
       "<span class=\"abstract-full has-text-grey-dark mathjax\" id=\"1901.08810v2-abstract-full\" style=\"display: none;\">\n",
       "        We consider the task of unsupervised extraction of meaningful latent representations of speech by applying autoencoding neural networks to speech waveforms. The goal is to learn a representation able to capture high level semantic content from the signal, e.g.\\ phoneme identities, while being invariant to confounding low level details in the signal such as the underlying pitch contour or background noise. Since the learned representation is tuned to contain only phonetic content, we resort to using a high capacity WaveNet decoder to infer information discarded by the encoder from previous samples. Moreover, the behavior of autoencoder models depends on the kind of constraint that is applied to the latent representation. We compare three variants: a simple dimensionality reduction bottleneck, a Gaussian Variational Autoencoder (<span class=\"search-hit mathjax\">VAE</span>), and a discrete Vector Quantized <span class=\"search-hit mathjax\">VAE</span> (<span class=\"search-hit mathjax\">VQ</span>-<span class=\"search-hit mathjax\">VAE</span>). We analyze the quality of learned representations in terms of speaker independence, the ability to predict phonetic content, and the ability to accurately reconstruct individual spectrogram frames. Moreover, for discrete encodings extracted using the <span class=\"search-hit mathjax\">VQ</span>-<span class=\"search-hit mathjax\">VAE</span>, we measure the ease of mapping them to phonemes. We introduce a regularization scheme that forces the representations to focus on the phonetic content of the utterance and report performance comparable with the top entries in the ZeroSpeech 2017 unsupervised acoustic unit discovery task.\n",
       "        <a class=\"is-size-7\" onclick=\"document.getElementById('1901.08810v2-abstract-full').style.display = 'none'; document.getElementById('1901.08810v2-abstract-short').style.display = 'inline';\" style=\"white-space: nowrap;\">△ Less</a>\n",
       "</span>\n",
       "</p>\n",
       "<p class=\"is-size-7\"><span class=\"has-text-black-bis has-text-weight-semibold\">Submitted</span> 11 September, 2019; <span class=\"has-text-black-bis has-text-weight-semibold\">v1</span> submitted 25 January, 2019;\n",
       "      <span class=\"has-text-black-bis has-text-weight-semibold\">originally announced</span> January 2019.\n",
       "      \n",
       "    </p>\n",
       "<p class=\"comments is-size-7\">\n",
       "<span class=\"has-text-black-bis has-text-weight-semibold\">Comments:</span>\n",
       "<span class=\"has-text-grey-dark mathjax\">Accepted to IEEE TASLP, final version available at http://dx.doi.org/10.1109/TASLP.2019.2938863</span>\n",
       "</p>\n",
       "</li>\n",
       "<li class=\"arxiv-result\">\n",
       "<div class=\"is-marginless\">\n",
       "<p class=\"list-title is-inline-block\"><a href=\"https://arxiv.org/abs/1811.05542\">arXiv:1811.05542</a>\n",
       "<span> [<a href=\"https://arxiv.org/pdf/1811.05542\">pdf</a>, <a href=\"https://arxiv.org/format/1811.05542\">other</a>] </span>\n",
       "</p>\n",
       "<div class=\"tags is-inline-block\">\n",
       "<span class=\"tag is-small is-link tooltip is-tooltip-top\" data-tooltip=\"Computation and Language\">cs.CL</span>\n",
       "<span class=\"tag is-small is-grey tooltip is-tooltip-top\" data-tooltip=\"Machine Learning\">cs.LG</span>\n",
       "<span class=\"tag is-small is-grey tooltip is-tooltip-top\" data-tooltip=\"Machine Learning\">stat.ML</span>\n",
       "</div>\n",
       "</div>\n",
       "<p class=\"title is-5 mathjax\">\n",
       "      \n",
       "        Extractive Summary as Discrete Latent Variables\n",
       "      \n",
       "    </p>\n",
       "<p class=\"authors\">\n",
       "<span class=\"has-text-black-bis has-text-weight-semibold\">Authors:</span>\n",
       "<a href=\"/search/?searchtype=author&amp;query=Komatsuzaki%2C+A\">Aran Komatsuzaki</a>\n",
       "</p>\n",
       "<p class=\"abstract mathjax\">\n",
       "<span class=\"search-hit\">Abstract</span>:\n",
       "      <span class=\"abstract-short has-text-grey-dark mathjax\" id=\"1811.05542v2-abstract-short\" style=\"display: inline;\">\n",
       "        …to compress a text using a neural model. We find that extracting tokens as latent variables significantly outperforms the state-of-the-art discrete latent variable models such as <span class=\"search-hit mathjax\">VQ</span>-<span class=\"search-hit mathjax\">VAE</span>. Furthermore, we compare various extractive compression schemes. There are two best-performing methods that perform equally. One metho…\n",
       "        <a class=\"is-size-7\" onclick=\"document.getElementById('1811.05542v2-abstract-full').style.display = 'inline'; document.getElementById('1811.05542v2-abstract-short').style.display = 'none';\" style=\"white-space: nowrap;\">▽ More</a>\n",
       "</span>\n",
       "<span class=\"abstract-full has-text-grey-dark mathjax\" id=\"1811.05542v2-abstract-full\" style=\"display: none;\">\n",
       "        In this paper, we compare various methods to compress a text using a neural model. We find that extracting tokens as latent variables significantly outperforms the state-of-the-art discrete latent variable models such as <span class=\"search-hit mathjax\">VQ</span>-<span class=\"search-hit mathjax\">VAE</span>. Furthermore, we compare various extractive compression schemes. There are two best-performing methods that perform equally. One method is to simply choose the tokens with the highest tf-idf scores. Another is to train a bidirectional language model similar to ELMo and choose the tokens with the highest loss. If we consider any subsequence of a text to be a text in a broader sense, we conclude that language is a strong compression code of itself. Our finding justifies the high quality of generation achieved with hierarchical method, as their latent variables are nothing but natural language summary. We also conclude that there is a hierarchy in language such that an entire text can be predicted much more easily based on a sequence of a small number of keywords, which can be easily found by classical methods as tf-idf. We speculate that this extraction process may be useful for unsupervised hierarchical text generation.\n",
       "        <a class=\"is-size-7\" onclick=\"document.getElementById('1811.05542v2-abstract-full').style.display = 'none'; document.getElementById('1811.05542v2-abstract-short').style.display = 'inline';\" style=\"white-space: nowrap;\">△ Less</a>\n",
       "</span>\n",
       "</p>\n",
       "<p class=\"is-size-7\"><span class=\"has-text-black-bis has-text-weight-semibold\">Submitted</span> 24 January, 2019; <span class=\"has-text-black-bis has-text-weight-semibold\">v1</span> submitted 13 November, 2018;\n",
       "      <span class=\"has-text-black-bis has-text-weight-semibold\">originally announced</span> November 2018.\n",
       "      \n",
       "    </p>\n",
       "</li>\n",
       "<li class=\"arxiv-result\">\n",
       "<div class=\"is-marginless\">\n",
       "<p class=\"list-title is-inline-block\"><a href=\"https://arxiv.org/abs/1808.01048\">arXiv:1808.01048</a>\n",
       "<span> [<a href=\"https://arxiv.org/pdf/1808.01048\">pdf</a>, <a href=\"https://arxiv.org/ps/1808.01048\">ps</a>, <a href=\"https://arxiv.org/format/1808.01048\">other</a>] </span>\n",
       "</p>\n",
       "<div class=\"tags is-inline-block\">\n",
       "<span class=\"tag is-small is-link tooltip is-tooltip-top\" data-tooltip=\"Machine Learning\">cs.LG</span>\n",
       "<span class=\"tag is-small is-grey tooltip is-tooltip-top\" data-tooltip=\"Machine Learning\">stat.ML</span>\n",
       "</div>\n",
       "</div>\n",
       "<p class=\"title is-5 mathjax\">\n",
       "      \n",
       "        Variational Information Bottleneck on Vector Quantized Autoencoders\n",
       "      \n",
       "    </p>\n",
       "<p class=\"authors\">\n",
       "<span class=\"has-text-black-bis has-text-weight-semibold\">Authors:</span>\n",
       "<a href=\"/search/?searchtype=author&amp;query=Wu%2C+H\">Hanwei Wu</a>, \n",
       "      \n",
       "      <a href=\"/search/?searchtype=author&amp;query=Flierl%2C+M\">Markus Flierl</a>\n",
       "</p>\n",
       "<p class=\"abstract mathjax\">\n",
       "<span class=\"search-hit\">Abstract</span>:\n",
       "      <span class=\"abstract-short has-text-grey-dark mathjax\" id=\"1808.01048v1-abstract-short\" style=\"display: inline;\">\n",
       "        In this paper, we provide an information-theoretic interpretation of the Vector Quantized-Variational Autoencoder (<span class=\"search-hit mathjax\">VQ</span>-…\n",
       "        <a class=\"is-size-7\" onclick=\"document.getElementById('1808.01048v1-abstract-full').style.display = 'inline'; document.getElementById('1808.01048v1-abstract-short').style.display = 'none';\" style=\"white-space: nowrap;\">▽ More</a>\n",
       "</span>\n",
       "<span class=\"abstract-full has-text-grey-dark mathjax\" id=\"1808.01048v1-abstract-full\" style=\"display: none;\">\n",
       "        In this paper, we provide an information-theoretic interpretation of the Vector Quantized-Variational Autoencoder (<span class=\"search-hit mathjax\">VQ</span>-<span class=\"search-hit mathjax\">VAE</span>). We show that the loss function of the original <span class=\"search-hit mathjax\">VQ</span>-<span class=\"search-hit mathjax\">VAE</span> can be derived from the variational deterministic information bottleneck (VDIB) principle. On the other hand, the <span class=\"search-hit mathjax\">VQ</span>-<span class=\"search-hit mathjax\">VAE</span> trained by the Expectation Maximization (EM) algorithm can be viewed as an approximation to the variational information bottleneck(VIB) principle.\n",
       "        <a class=\"is-size-7\" onclick=\"document.getElementById('1808.01048v1-abstract-full').style.display = 'none'; document.getElementById('1808.01048v1-abstract-short').style.display = 'inline';\" style=\"white-space: nowrap;\">△ Less</a>\n",
       "</span>\n",
       "</p>\n",
       "<p class=\"is-size-7\"><span class=\"has-text-black-bis has-text-weight-semibold\">Submitted</span> 2 August, 2018; \n",
       "      <span class=\"has-text-black-bis has-text-weight-semibold\">originally announced</span> August 2018.\n",
       "      \n",
       "    </p>\n",
       "</li>\n",
       "<li class=\"arxiv-result\">\n",
       "<div class=\"is-marginless\">\n",
       "<p class=\"list-title is-inline-block\"><a href=\"https://arxiv.org/abs/1807.11470\">arXiv:1807.11470</a>\n",
       "<span> [<a href=\"https://arxiv.org/pdf/1807.11470\">pdf</a>, <a href=\"https://arxiv.org/format/1807.11470\">other</a>] </span>\n",
       "</p>\n",
       "<div class=\"tags is-inline-block\">\n",
       "<span class=\"tag is-small is-link tooltip is-tooltip-top\" data-tooltip=\"Audio and Speech Processing\">eess.AS</span>\n",
       "<span class=\"tag is-small is-grey tooltip is-tooltip-top\" data-tooltip=\"Machine Learning\">cs.LG</span>\n",
       "<span class=\"tag is-small is-grey tooltip is-tooltip-top\" data-tooltip=\"Sound\">cs.SD</span>\n",
       "<span class=\"tag is-small is-grey tooltip is-tooltip-top\" data-tooltip=\"Machine Learning\">stat.ML</span>\n",
       "</div>\n",
       "</div>\n",
       "<p class=\"title is-5 mathjax\">\n",
       "      \n",
       "        Deep Encoder-Decoder Models for Unsupervised Learning of Controllable Speech Synthesis\n",
       "      \n",
       "    </p>\n",
       "<p class=\"authors\">\n",
       "<span class=\"has-text-black-bis has-text-weight-semibold\">Authors:</span>\n",
       "<a href=\"/search/?searchtype=author&amp;query=Henter%2C+G+E\">Gustav Eje Henter</a>, \n",
       "      \n",
       "      <a href=\"/search/?searchtype=author&amp;query=Lorenzo-Trueba%2C+J\">Jaime Lorenzo-Trueba</a>, \n",
       "      \n",
       "      <a href=\"/search/?searchtype=author&amp;query=Wang%2C+X\">Xin Wang</a>, \n",
       "      \n",
       "      <a href=\"/search/?searchtype=author&amp;query=Yamagishi%2C+J\">Junichi Yamagishi</a>\n",
       "</p>\n",
       "<p class=\"abstract mathjax\">\n",
       "<span class=\"search-hit\">Abstract</span>:\n",
       "      <span class=\"abstract-short has-text-grey-dark mathjax\" id=\"1807.11470v3-abstract-short\" style=\"display: inline;\">\n",
       "        …example, we show that popular unsupervised training heuristics can be interpreted as variational inference in certain autoencoder models. We additionally connect these models to <span class=\"search-hit mathjax\">VQ</span>-<span class=\"search-hit mathjax\">VAEs</span>, another, recently-proposed class of deep variational autoencoders, which we show can be derived from a very similar mathematical argu…\n",
       "        <a class=\"is-size-7\" onclick=\"document.getElementById('1807.11470v3-abstract-full').style.display = 'inline'; document.getElementById('1807.11470v3-abstract-short').style.display = 'none';\" style=\"white-space: nowrap;\">▽ More</a>\n",
       "</span>\n",
       "<span class=\"abstract-full has-text-grey-dark mathjax\" id=\"1807.11470v3-abstract-full\" style=\"display: none;\">\n",
       "        Generating versatile and appropriate synthetic speech requires control over the output expression separate from the spoken text. Important non-textual speech variation is seldom annotated, in which case output control must be learned in an unsupervised fashion. In this paper, we perform an in-depth study of methods for unsupervised learning of control in statistical speech synthesis. For example, we show that popular unsupervised training heuristics can be interpreted as variational inference in certain autoencoder models. We additionally connect these models to <span class=\"search-hit mathjax\">VQ</span>-<span class=\"search-hit mathjax\">VAEs</span>, another, recently-proposed class of deep variational autoencoders, which we show can be derived from a very similar mathematical argument. The implications of these new probabilistic interpretations are discussed. We illustrate the utility of the various approaches with an application to acoustic modelling for emotional speech synthesis, where the unsupervised methods for learning expression control (without access to emotional labels) are found to give results that in many aspects match or surpass the previous best supervised approach.\n",
       "        <a class=\"is-size-7\" onclick=\"document.getElementById('1807.11470v3-abstract-full').style.display = 'none'; document.getElementById('1807.11470v3-abstract-short').style.display = 'inline';\" style=\"white-space: nowrap;\">△ Less</a>\n",
       "</span>\n",
       "</p>\n",
       "<p class=\"is-size-7\"><span class=\"has-text-black-bis has-text-weight-semibold\">Submitted</span> 9 September, 2018; <span class=\"has-text-black-bis has-text-weight-semibold\">v1</span> submitted 30 July, 2018;\n",
       "      <span class=\"has-text-black-bis has-text-weight-semibold\">originally announced</span> July 2018.\n",
       "      \n",
       "    </p>\n",
       "<p class=\"comments is-size-7\">\n",
       "<span class=\"has-text-black-bis has-text-weight-semibold\">Comments:</span>\n",
       "<span class=\"has-text-grey-dark mathjax\">17 pages, 4 figures</span>\n",
       "</p>\n",
       "<p class=\"comments is-size-7\">\n",
       "<span class=\"has-text-black-bis has-text-weight-semibold\">MSC Class:</span>\n",
       "          62F99\n",
       "        \n",
       "\n",
       "        \n",
       "          <span class=\"has-text-black-bis has-text-weight-semibold\">ACM Class:</span>\n",
       "          I.2.7; G.3\n",
       "        \n",
       "      </p>\n",
       "</li>\n",
       "<li class=\"arxiv-result\">\n",
       "<div class=\"is-marginless\">\n",
       "<p class=\"list-title is-inline-block\"><a href=\"https://arxiv.org/abs/1807.04629\">arXiv:1807.04629</a>\n",
       "<span> [<a href=\"https://arxiv.org/pdf/1807.04629\">pdf</a>, <a href=\"https://arxiv.org/format/1807.04629\">other</a>] </span>\n",
       "</p>\n",
       "<div class=\"tags is-inline-block\">\n",
       "<span class=\"tag is-small is-link tooltip is-tooltip-top\" data-tooltip=\"Image and Video Processing\">eess.IV</span>\n",
       "<span class=\"tag is-small is-grey tooltip is-tooltip-top\" data-tooltip=\"Computer Vision and Pattern Recognition\">cs.CV</span>\n",
       "<span class=\"tag is-small is-grey tooltip is-tooltip-top\" data-tooltip=\"Machine Learning\">cs.LG</span>\n",
       "</div>\n",
       "</div>\n",
       "<p class=\"title is-5 mathjax\">\n",
       "      \n",
       "        Learning Product Codebooks using Vector Quantized Autoencoders for Image Retrieval\n",
       "      \n",
       "    </p>\n",
       "<p class=\"authors\">\n",
       "<span class=\"has-text-black-bis has-text-weight-semibold\">Authors:</span>\n",
       "<a href=\"/search/?searchtype=author&amp;query=Wu%2C+H\">Hanwei Wu</a>, \n",
       "      \n",
       "      <a href=\"/search/?searchtype=author&amp;query=Flierl%2C+M\">Markus Flierl</a>\n",
       "</p>\n",
       "<p class=\"abstract mathjax\">\n",
       "<span class=\"search-hit\">Abstract</span>:\n",
       "      <span class=\"abstract-short has-text-grey-dark mathjax\" id=\"1807.04629v4-abstract-short\" style=\"display: inline;\">\n",
       "        Vector-Quantized Variational Autoencoders (<span class=\"search-hit mathjax\">VQ</span>-…\n",
       "        <a class=\"is-size-7\" onclick=\"document.getElementById('1807.04629v4-abstract-full').style.display = 'inline'; document.getElementById('1807.04629v4-abstract-short').style.display = 'none';\" style=\"white-space: nowrap;\">▽ More</a>\n",
       "</span>\n",
       "<span class=\"abstract-full has-text-grey-dark mathjax\" id=\"1807.04629v4-abstract-full\" style=\"display: none;\">\n",
       "        Vector-Quantized Variational Autoencoders (<span class=\"search-hit mathjax\">VQ</span>-<span class=\"search-hit mathjax\">VAE</span>)[1] provide an unsupervised model for learning discrete representations by combining vector quantization and autoencoders. In this paper, we study the use of <span class=\"search-hit mathjax\">VQ</span>-<span class=\"search-hit mathjax\">VAE</span> for representation learning for downstream tasks, such as image retrieval. We first describe the <span class=\"search-hit mathjax\">VQ</span>-<span class=\"search-hit mathjax\">VAE</span> in the context of an information-theoretic framework. We show that the regularization term on the learned representation is determined by the size of the embedded codebook before the training and it affects the generalization ability of the model. As a result, we introduce a hyperparameter to balance the strength of the vector quantizer and the reconstruction error. By tuning the hyperparameter, the embedded bottleneck quantizer is used as a regularizer that forces the output of the encoder to share a constrained coding space such that learned latent features preserve the similarity relations of the data space. In addition, we provide a search range for finding the best hyperparameter. Finally, we incorporate the product quantization into the bottleneck stage of <span class=\"search-hit mathjax\">VQ</span>-<span class=\"search-hit mathjax\">VAE</span> and propose an end-to-end unsupervised learning model for the image retrieval task. The product quantizer has the advantage of generating large-size codebooks. Fast retrieval can be achieved by using the lookup tables that store the distance between any pair of sub-codewords. State-of-the-art retrieval results are achieved by the learned codebooks.\n",
       "        <a class=\"is-size-7\" onclick=\"document.getElementById('1807.04629v4-abstract-full').style.display = 'none'; document.getElementById('1807.04629v4-abstract-short').style.display = 'inline';\" style=\"white-space: nowrap;\">△ Less</a>\n",
       "</span>\n",
       "</p>\n",
       "<p class=\"is-size-7\"><span class=\"has-text-black-bis has-text-weight-semibold\">Submitted</span> 4 March, 2019; <span class=\"has-text-black-bis has-text-weight-semibold\">v1</span> submitted 12 July, 2018;\n",
       "      <span class=\"has-text-black-bis has-text-weight-semibold\">originally announced</span> July 2018.\n",
       "      \n",
       "    </p>\n",
       "</li>\n",
       "<li class=\"arxiv-result\">\n",
       "<div class=\"is-marginless\">\n",
       "<p class=\"list-title is-inline-block\"><a href=\"https://arxiv.org/abs/1805.11063\">arXiv:1805.11063</a>\n",
       "<span> [<a href=\"https://arxiv.org/pdf/1805.11063\">pdf</a>, <a href=\"https://arxiv.org/format/1805.11063\">other</a>] </span>\n",
       "</p>\n",
       "<div class=\"tags is-inline-block\">\n",
       "<span class=\"tag is-small is-link tooltip is-tooltip-top\" data-tooltip=\"Machine Learning\">cs.LG</span>\n",
       "<span class=\"tag is-small is-grey tooltip is-tooltip-top\" data-tooltip=\"Machine Learning\">stat.ML</span>\n",
       "</div>\n",
       "</div>\n",
       "<p class=\"title is-5 mathjax\">\n",
       "      \n",
       "        Theory and Experiments on Vector Quantized Autoencoders\n",
       "      \n",
       "    </p>\n",
       "<p class=\"authors\">\n",
       "<span class=\"has-text-black-bis has-text-weight-semibold\">Authors:</span>\n",
       "<a href=\"/search/?searchtype=author&amp;query=Roy%2C+A\">Aurko Roy</a>, \n",
       "      \n",
       "      <a href=\"/search/?searchtype=author&amp;query=Vaswani%2C+A\">Ashish Vaswani</a>, \n",
       "      \n",
       "      <a href=\"/search/?searchtype=author&amp;query=Neelakantan%2C+A\">Arvind Neelakantan</a>, \n",
       "      \n",
       "      <a href=\"/search/?searchtype=author&amp;query=Parmar%2C+N\">Niki Parmar</a>\n",
       "</p>\n",
       "<p class=\"abstract mathjax\">\n",
       "<span class=\"search-hit\">Abstract</span>:\n",
       "      <span class=\"abstract-short has-text-grey-dark mathjax\" id=\"1805.11063v2-abstract-short\" style=\"display: inline;\">\n",
       "        …latent variable models has remained challenging and their performance has mostly failed to match their continuous counterparts. Recent work on vector quantized autoencoders (<span class=\"search-hit mathjax\">VQ</span>-…\n",
       "        <a class=\"is-size-7\" onclick=\"document.getElementById('1805.11063v2-abstract-full').style.display = 'inline'; document.getElementById('1805.11063v2-abstract-short').style.display = 'none';\" style=\"white-space: nowrap;\">▽ More</a>\n",
       "</span>\n",
       "<span class=\"abstract-full has-text-grey-dark mathjax\" id=\"1805.11063v2-abstract-full\" style=\"display: none;\">\n",
       "        Deep neural networks with discrete latent variables offer the promise of better symbolic reasoning, and learning abstractions that are more useful to new tasks. There has been a surge in interest in discrete latent variable models, however, despite several recent improvements, the training of discrete latent variable models has remained challenging and their performance has mostly failed to match their continuous counterparts. Recent work on vector quantized autoencoders (<span class=\"search-hit mathjax\">VQ</span>-<span class=\"search-hit mathjax\">VAE</span>) has made substantial progress in this direction, with its perplexity almost matching that of a <span class=\"search-hit mathjax\">VAE</span> on datasets such as CIFAR-10. In this work, we investigate an alternate training technique for <span class=\"search-hit mathjax\">VQ</span>-<span class=\"search-hit mathjax\">VAE</span>, inspired by its connection to the Expectation Maximization (EM) algorithm. Training the discrete bottleneck with EM helps us achieve better image generation results on CIFAR-10, and together with knowledge distillation, allows us to develop a non-autoregressive machine translation model whose accuracy almost matches a strong greedy autoregressive baseline Transformer, while being 3.3 times faster at inference.\n",
       "        <a class=\"is-size-7\" onclick=\"document.getElementById('1805.11063v2-abstract-full').style.display = 'none'; document.getElementById('1805.11063v2-abstract-short').style.display = 'inline';\" style=\"white-space: nowrap;\">△ Less</a>\n",
       "</span>\n",
       "</p>\n",
       "<p class=\"is-size-7\"><span class=\"has-text-black-bis has-text-weight-semibold\">Submitted</span> 20 July, 2018; <span class=\"has-text-black-bis has-text-weight-semibold\">v1</span> submitted 28 May, 2018;\n",
       "      <span class=\"has-text-black-bis has-text-weight-semibold\">originally announced</span> May 2018.\n",
       "      \n",
       "    </p>\n",
       "</li>\n",
       "<li class=\"arxiv-result\">\n",
       "<div class=\"is-marginless\">\n",
       "<p class=\"list-title is-inline-block\"><a href=\"https://arxiv.org/abs/1711.00937\">arXiv:1711.00937</a>\n",
       "<span> [<a href=\"https://arxiv.org/pdf/1711.00937\">pdf</a>, <a href=\"https://arxiv.org/format/1711.00937\">other</a>] </span>\n",
       "</p>\n",
       "<div class=\"tags is-inline-block\">\n",
       "<span class=\"tag is-small is-link tooltip is-tooltip-top\" data-tooltip=\"Learning\">cs.LG</span>\n",
       "</div>\n",
       "</div>\n",
       "<p class=\"title is-5 mathjax\">\n",
       "      \n",
       "        Neural Discrete Representation Learning\n",
       "      \n",
       "    </p>\n",
       "<p class=\"authors\">\n",
       "<span class=\"has-text-black-bis has-text-weight-semibold\">Authors:</span>\n",
       "<a href=\"/search/?searchtype=author&amp;query=Oord%2C+A+v+d\">Aaron van den Oord</a>, \n",
       "      \n",
       "      <a href=\"/search/?searchtype=author&amp;query=Vinyals%2C+O\">Oriol Vinyals</a>, \n",
       "      \n",
       "      <a href=\"/search/?searchtype=author&amp;query=Kavukcuoglu%2C+K\">Koray Kavukcuoglu</a>\n",
       "</p>\n",
       "<p class=\"abstract mathjax\">\n",
       "<span class=\"search-hit\">Abstract</span>:\n",
       "      <span class=\"abstract-short has-text-grey-dark mathjax\" id=\"1711.00937v2-abstract-short\" style=\"display: inline;\">\n",
       "        …learning. In this paper, we propose a simple yet powerful generative model that learns such discrete representations. Our model, the Vector Quantised-Variational AutoEncoder (<span class=\"search-hit mathjax\">VQ</span>-…\n",
       "        <a class=\"is-size-7\" onclick=\"document.getElementById('1711.00937v2-abstract-full').style.display = 'inline'; document.getElementById('1711.00937v2-abstract-short').style.display = 'none';\" style=\"white-space: nowrap;\">▽ More</a>\n",
       "</span>\n",
       "<span class=\"abstract-full has-text-grey-dark mathjax\" id=\"1711.00937v2-abstract-full\" style=\"display: none;\">\n",
       "        Learning useful representations without supervision remains a key challenge in machine learning. In this paper, we propose a simple yet powerful generative model that learns such discrete representations. Our model, the Vector Quantised-Variational AutoEncoder (<span class=\"search-hit mathjax\">VQ</span>-<span class=\"search-hit mathjax\">VAE</span>), differs from <span class=\"search-hit mathjax\">VAEs</span> in two key ways: the encoder network outputs discrete, rather than continuous, codes; and the prior is learnt rather than static. In order to learn a discrete latent representation, we incorporate ideas from vector quantisation (<span class=\"search-hit mathjax\">VQ</span>). Using the <span class=\"search-hit mathjax\">VQ</span> method allows the model to circumvent issues of \"posterior collapse\" -- where the latents are ignored when they are paired with a powerful autoregressive decoder -- typically observed in the <span class=\"search-hit mathjax\">VAE</span> framework. Pairing these representations with an autoregressive prior, the model can generate high quality images, videos, and speech as well as doing high quality speaker conversion and unsupervised learning of phonemes, providing further evidence of the utility of the learnt representations.\n",
       "        <a class=\"is-size-7\" onclick=\"document.getElementById('1711.00937v2-abstract-full').style.display = 'none'; document.getElementById('1711.00937v2-abstract-short').style.display = 'inline';\" style=\"white-space: nowrap;\">△ Less</a>\n",
       "</span>\n",
       "</p>\n",
       "<p class=\"is-size-7\"><span class=\"has-text-black-bis has-text-weight-semibold\">Submitted</span> 30 May, 2018; <span class=\"has-text-black-bis has-text-weight-semibold\">v1</span> submitted 2 November, 2017;\n",
       "      <span class=\"has-text-black-bis has-text-weight-semibold\">originally announced</span> November 2017.\n",
       "      \n",
       "    </p>\n",
       "</li>\n",
       "</ol>\n",
       "<div class=\"is-hidden-tablet\">\n",
       "<!-- feedback for mobile only -->\n",
       "<span class=\"help\" style=\"display: inline-block;\"><a href=\"https://github.com/arXiv/arxiv-search/releases\">Search v0.5.6 released 2020-02-24</a>  </span>\n",
       "<button class=\"button is-small\" id=\"feedback-button\">Feedback?</button>\n",
       "</div>\n",
       "</div>\n",
       "</main>\n",
       "<footer>\n",
       "<div aria-label=\"Secondary\" class=\"columns is-desktop\" role=\"navigation\">\n",
       "<!-- MetaColumn 1 -->\n",
       "<div class=\"column\">\n",
       "<div class=\"columns\">\n",
       "<div class=\"column\">\n",
       "<ul class=\"nav-spaced\">\n",
       "<li><a href=\"https://arxiv.org/about\">About</a></li>\n",
       "<li><a href=\"https://arxiv.org/help\">Help</a></li>\n",
       "</ul>\n",
       "</div>\n",
       "<div class=\"column\">\n",
       "<ul class=\"nav-spaced\">\n",
       "<li>\n",
       "<svg class=\"icon filter-black\" role=\"presentation\" viewbox=\"0 0 512 512\" xmlns=\"http://www.w3.org/2000/svg\"><title>contact arXiv</title><desc>Click here to contact arXiv</desc><path d=\"M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z\"></path></svg>\n",
       "<a href=\"https://arxiv.org/help/contact\"> Contact</a>\n",
       "</li>\n",
       "<li>\n",
       "<svg class=\"icon filter-black\" role=\"presentation\" viewbox=\"0 0 512 512\" xmlns=\"http://www.w3.org/2000/svg\"><title>subscribe to arXiv mailings</title><desc>Click here to subscribe</desc><path d=\"M476 3.2L12.5 270.6c-18.1 10.4-15.8 35.6 2.2 43.2L121 358.4l287.3-253.2c5.5-4.9 13.3 2.6 8.6 8.3L176 407v80.5c0 23.6 28.5 32.9 42.5 15.8L282 426l124.6 52.2c14.2 6 30.4-2.9 33-18.2l72-432C515 7.8 493.3-6.8 476 3.2z\"></path></svg>\n",
       "<a href=\"https://arxiv.org/help/subscribe\"> Subscribe</a>\n",
       "</li>\n",
       "</ul>\n",
       "</div>\n",
       "</div>\n",
       "</div> <!-- end MetaColumn 1 -->\n",
       "<!-- MetaColumn 2 -->\n",
       "<div class=\"column\">\n",
       "<div class=\"columns\">\n",
       "<div class=\"column\">\n",
       "<ul class=\"nav-spaced\">\n",
       "<li><a href=\"https://arxiv.org/help/license\">Copyright</a></li>\n",
       "<li><a href=\"https://arxiv.org/help/policies/privacy_policy\">Privacy Policy</a></li>\n",
       "</ul>\n",
       "</div>\n",
       "<div class=\"column sorry-app-links\">\n",
       "<ul class=\"nav-spaced\">\n",
       "<li><a href=\"https://arxiv.org/help/web_accessibility\">Web Accessibility Assistance</a></li>\n",
       "<li>\n",
       "<p class=\"help\">\n",
       "<a class=\"a11y-main-link\" href=\"https://status.arxiv.org\" target=\"_blank\">arXiv Operational Status <svg class=\"icon filter-dark_grey\" role=\"presentation\" viewbox=\"0 0 256 512\" xmlns=\"http://www.w3.org/2000/svg\"><path d=\"M224.3 273l-136 136c-9.4 9.4-24.6 9.4-33.9 0l-22.6-22.6c-9.4-9.4-9.4-24.6 0-33.9l96.4-96.4-96.4-96.4c-9.4-9.4-9.4-24.6 0-33.9L54.3 103c9.4-9.4 24.6-9.4 33.9 0l136 136c9.5 9.4 9.5 24.6.1 34z\"></path></svg></a><br/>\n",
       "              Get status notifications via\n",
       "              <a class=\"is-link\" href=\"https://subscribe.sorryapp.com/24846f03/email/new\" target=\"_blank\"><svg class=\"icon filter-black\" role=\"presentation\" viewbox=\"0 0 512 512\" xmlns=\"http://www.w3.org/2000/svg\"><path d=\"M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z\"></path></svg>email</a>\n",
       "              or <a class=\"is-link\" href=\"https://subscribe.sorryapp.com/24846f03/slack/new\" target=\"_blank\"><svg class=\"icon filter-black\" role=\"presentation\" viewbox=\"0 0 448 512\" xmlns=\"http://www.w3.org/2000/svg\"><path d=\"M94.12 315.1c0 25.9-21.16 47.06-47.06 47.06S0 341 0 315.1c0-25.9 21.16-47.06 47.06-47.06h47.06v47.06zm23.72 0c0-25.9 21.16-47.06 47.06-47.06s47.06 21.16 47.06 47.06v117.84c0 25.9-21.16 47.06-47.06 47.06s-47.06-21.16-47.06-47.06V315.1zm47.06-188.98c-25.9 0-47.06-21.16-47.06-47.06S139 32 164.9 32s47.06 21.16 47.06 47.06v47.06H164.9zm0 23.72c25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06H47.06C21.16 243.96 0 222.8 0 196.9s21.16-47.06 47.06-47.06H164.9zm188.98 47.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06h-47.06V196.9zm-23.72 0c0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06V79.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06V196.9zM283.1 385.88c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06v-47.06h47.06zm0-23.72c-25.9 0-47.06-21.16-47.06-47.06 0-25.9 21.16-47.06 47.06-47.06h117.84c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06H283.1z\"></path></svg>slack</a>\n",
       "</p>\n",
       "</li>\n",
       "</ul>\n",
       "</div>\n",
       "</div>\n",
       "</div> <!-- end MetaColumn 2 -->\n",
       "</div>\n",
       "</footer>\n",
       "</body>\n",
       "</html>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page_soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 - Extraction of the web data (Web Scraping or Web Data extraction)\n",
    "> Based on the type and volume of data, the data extraction step plays a very crucial role.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the search results from the website are now stored in the ind_doc_sec variable\n",
    "\n",
    "ind_doc_sec = page_soup.findAll(\"li\",{\"class\":\"arxiv-result\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upon checking the website, it primarily has the following sections:\n",
    "> 1. `Doc_id`\n",
    "> 2. `Tags`\n",
    "> 3. `Doc_title`\n",
    "> 4. `Author`\n",
    "> 5. `Abstract of the document`\n",
    "> 6. `Submission date`\n",
    "> 7. `Original Announcement date`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Each of the items can be identified and extracted by pulling the respective class information from the html code.\n",
    "> 1. `Doc_id` is present inside the \"p\"- {\"class\":\"list-title is-inline-block\"} which subsequently is present inside the \"div\" - {\"class\":\"is-marginless\"}\n",
    "> 2. `Tags` is present inside the \"span\" - {\"class\":\"tag is-small is-grey tooltip is-tooltip-top\" which is subsequently present inside the \"div\" - {\"class\":\"tags is-inline-block\"}\n",
    "> 3. `Doc_title` is present inside the \"p\" - {\"class\":\"title is-5 mathjax\"\n",
    "> 4. `Authors` is present inside the \"p\" - {\"class\":\"authors\"}\n",
    "> 5. `Abstract` is present inside the \"span\" - {\"class\":\"abstract-full has-text-grey-dark mathjax\"}\n",
    "> 6. `Submitted` is present inside the \"p\" - {\"class\":\"is-size-7\"}\n",
    "> 7. `Originally Announced` is present inside the \"p\" - {\"class\":\"is-size-7\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing a dictionary to store the data\n",
    "data_final ={\"Doc_id\":[],\n",
    "             \"Tags\":[],\n",
    "            \"Doc_title\":[],\n",
    "            \"Authors\":[],\n",
    "            \"Abstract\":[],\n",
    "            \"Submitted\":[],\n",
    "            \"Originally Announced\":[]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining a loop to extract the items from each section of the results\n",
    "for i in range(len(ind_doc_sec)):\n",
    "    doc_id= ind_doc_sec[i].find(\"div\",{\"class\":\"is-marginless\"}).find(\"p\",{\"class\":\"list-title is-inline-block\"}).text.strip().split('\\n\\xa0')[0]\n",
    "    data_final[\"Doc_id\"].append(doc_id)\n",
    "    data_tag =[]\n",
    "    for x in range(len(ind_doc_sec[i].find(\"div\",{\"class\":\"tags is-inline-block\"}).findAll(\"span\",{\"class\":\"tag is-small is-grey tooltip is-tooltip-top\"}))):\n",
    "        data_tag.append(ind_doc_sec[i].find(\"div\",{\"class\":\"tags is-inline-block\"}).findAll(\"span\",{\"class\":\"tag is-small is-grey tooltip is-tooltip-top\"})[x].text)\n",
    "    data_final[\"Tags\"].append(data_tag)\n",
    "    data_final[\"Doc_title\"].append(ind_doc_sec[i].find(\"p\",{\"class\":\"title is-5 mathjax\"}).text.strip())\n",
    "    data_authors = []\n",
    "    for y in range(len(ind_doc_sec[i].find(\"p\",{\"class\":\"authors\"}).findAll(\"a\"))):\n",
    "        data_authors.append(ind_doc_sec[i].find(\"p\",{\"class\":\"authors\"}).findAll(\"a\")[y].text)\n",
    "    data_final[\"Authors\"].append(data_authors)\n",
    "    data_final[\"Abstract\"].append(ind_doc_sec[i].find(\"span\",{\"class\":\"abstract-full has-text-grey-dark mathjax\"}).text.strip())\n",
    "    data_final[\"Submitted\"].append(ind_doc_sec[i].find(\"p\",{\"class\":\"is-size-7\"}).text.strip().split('\\n')[0].replace('Submitted ',''))\n",
    "    data_final[\"Originally Announced\"].append(ind_doc_sec[i].find(\"p\",{\"class\":\"is-size-7\"}).text.strip().split('\\n')[1].replace('      originally announced ',''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Doc_id</th>\n",
       "      <th>Tags</th>\n",
       "      <th>Doc_title</th>\n",
       "      <th>Authors</th>\n",
       "      <th>Abstract</th>\n",
       "      <th>Submitted</th>\n",
       "      <th>Originally Announced</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>arXiv:2008.04549</td>\n",
       "      <td>[cs.SD]</td>\n",
       "      <td>Unsupervised Learning For Sequence-to-sequence...</td>\n",
       "      <td>[Haitong Zhang, Yue Lin]</td>\n",
       "      <td>Recently, sequence-to-sequence models with att...</td>\n",
       "      <td>11 August, 2020;</td>\n",
       "      <td>August 2020.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>arXiv:2008.02528</td>\n",
       "      <td>[stat.ML]</td>\n",
       "      <td>Learning Sampling in Financial Statement Audit...</td>\n",
       "      <td>[Marco Schreyer, Timur Sattarov, Anita Gierbl,...</td>\n",
       "      <td>The audit of financial statements is designed ...</td>\n",
       "      <td>6 August, 2020;</td>\n",
       "      <td>August 2020.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>arXiv:2007.09923</td>\n",
       "      <td>[cs.LG, eess.IV]</td>\n",
       "      <td>Incorporating Reinforced Adversarial Learning ...</td>\n",
       "      <td>[Kenan E. Ak, Ning Xu, Zhe Lin, Yilin Wang]</td>\n",
       "      <td>Autoregressive models recently achieved compar...</td>\n",
       "      <td>20 July, 2020;</td>\n",
       "      <td>July 2020.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>arXiv:2006.12150</td>\n",
       "      <td>[]</td>\n",
       "      <td>Generating Annotated High-Fidelity Images Cont...</td>\n",
       "      <td>[Bryan G. Cardenas, Devanshu Arya, Deepak K. G...</td>\n",
       "      <td>Recent developments related to generative mode...</td>\n",
       "      <td>24 June, 2020; v1 submitted 22 June, 2020;</td>\n",
       "      <td>June 2020.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>arXiv:2006.07926</td>\n",
       "      <td>[cs.CL]</td>\n",
       "      <td>UWSpeech: Speech to Speech Translation for Unw...</td>\n",
       "      <td>[Chen Zhang, Xu Tan, Yi Ren, Tao Qin, Kejun Zh...</td>\n",
       "      <td>Existing speech to speech translation systems ...</td>\n",
       "      <td>14 June, 2020;</td>\n",
       "      <td>June 2020.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Doc_id              Tags  \\\n",
       "0  arXiv:2008.04549           [cs.SD]   \n",
       "1  arXiv:2008.02528         [stat.ML]   \n",
       "2  arXiv:2007.09923  [cs.LG, eess.IV]   \n",
       "3  arXiv:2006.12150                []   \n",
       "4  arXiv:2006.07926           [cs.CL]   \n",
       "\n",
       "                                           Doc_title  \\\n",
       "0  Unsupervised Learning For Sequence-to-sequence...   \n",
       "1  Learning Sampling in Financial Statement Audit...   \n",
       "2  Incorporating Reinforced Adversarial Learning ...   \n",
       "3  Generating Annotated High-Fidelity Images Cont...   \n",
       "4  UWSpeech: Speech to Speech Translation for Unw...   \n",
       "\n",
       "                                             Authors  \\\n",
       "0                           [Haitong Zhang, Yue Lin]   \n",
       "1  [Marco Schreyer, Timur Sattarov, Anita Gierbl,...   \n",
       "2        [Kenan E. Ak, Ning Xu, Zhe Lin, Yilin Wang]   \n",
       "3  [Bryan G. Cardenas, Devanshu Arya, Deepak K. G...   \n",
       "4  [Chen Zhang, Xu Tan, Yi Ren, Tao Qin, Kejun Zh...   \n",
       "\n",
       "                                            Abstract  \\\n",
       "0  Recently, sequence-to-sequence models with att...   \n",
       "1  The audit of financial statements is designed ...   \n",
       "2  Autoregressive models recently achieved compar...   \n",
       "3  Recent developments related to generative mode...   \n",
       "4  Existing speech to speech translation systems ...   \n",
       "\n",
       "                                    Submitted Originally Announced  \n",
       "0                           11 August, 2020;          August 2020.  \n",
       "1                            6 August, 2020;          August 2020.  \n",
       "2                             20 July, 2020;            July 2020.  \n",
       "3  24 June, 2020; v1 submitted 22 June, 2020;           June 2020.  \n",
       "4                             14 June, 2020;            June 2020.  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# converting the data into a dataframe\n",
    "data_df1 = pd.DataFrame(data_final)\n",
    "data_df1.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
